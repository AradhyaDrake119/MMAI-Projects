{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable-baselines3==0.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFV2shAkwnPB",
        "outputId": "d5b6a609-5022-4889-ca7c-725a97f4b501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/953.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.0\n",
            "Collecting stable-baselines3==0.11.0\n",
            "  Downloading stable_baselines3-0.11.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.1/152.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==0.11.0) (3.7.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17->stable-baselines3==0.11.0) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->stable-baselines3==0.11.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->stable-baselines3==0.11.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->stable-baselines3==0.11.0) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==0.11.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==0.11.0) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==0.11.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->stable-baselines3==0.11.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->stable-baselines3==0.11.0) (1.3.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_TKfhOywOOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9116a497-cd7b-4da2-9834-b1245a4ce705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gym version:\", gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cPva0nttQZH",
        "outputId": "afee3541-7af5-42eb-e2d3-851ca76e8a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gym version: 0.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG4NQpSqwt-Q",
        "outputId": "4c93ec2f-1244-474b-e6b1-2224aed08601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
            "  self._read_thread.setDaemon(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMountainCarContinuous(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.prev_position = None  # Initialize previous position\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "        # Your existing energy cost calculation\n",
        "        m = 1.0  # Vehicle mass\n",
        "        g = 9.81  # Acceleration due to gravity\n",
        "        theta = np.arctan(3 * next_state[0])  # Road gradient\n",
        "        Cd = 0.3  # Drag coefficient\n",
        "        A = 2.0  # Vehicle frontal area\n",
        "        rho = 1.2  # Air density\n",
        "        Cr = 0.02  # Rolling resistance coefficient\n",
        "        velocity = next_state[1]\n",
        "        F_gravity = m * g * np.sin(theta)\n",
        "        F_drag = 0.5 * Cd * A * rho * velocity**2\n",
        "        F_rolling = m * g * Cr * np.cos(theta)\n",
        "        P = (F_gravity + F_drag + F_rolling) * velocity\n",
        "        action_scalar = action.item() if isinstance(action, np.ndarray) else action\n",
        "        energy_cost = P * np.abs(action_scalar)\n",
        "        reward -= energy_cost\n",
        "\n",
        "        # Add reward for moving closer to the goal\n",
        "        goal_position = 0.5  # Modify as needed\n",
        "        if self.prev_position is not None:\n",
        "            # Increase reward if moving closer to the goal\n",
        "            if np.abs(next_state[0] - goal_position) < np.abs(self.prev_position - goal_position):\n",
        "                reward += 0.1  # Modify as needed\n",
        "        self.prev_position = next_state[0]\n",
        "\n",
        "        return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "53Q7IYGDuuuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionMonitor(Monitor):\n",
        "    def __init__(self, env, filename):\n",
        "        super().__init__(env, filename)\n",
        "        self.action_sum = 0\n",
        "        self.action_sums = []\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.action_sum = 0\n",
        "        return super().reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, info = super().step(action)\n",
        "        self.action_sum += np.sum(np.abs(action))\n",
        "        if done:\n",
        "            self.action_sums.append(self.action_sum)\n",
        "        return observation, reward, done, info"
      ],
      "metadata": {
        "id": "nhVqeD255W1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Ensure the log directory exists\n",
        "if not os.path.exists('./logs/'):\n",
        "    os.makedirs('./logs/')"
      ],
      "metadata": {
        "id": "mjPt78h5QNi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = ActionMonitor(CustomMountainCarContinuous(gym.make('MountainCarContinuous-v0')), filename=\"./logs/MountainCarContinuous-v0_1.monitor.csv\")"
      ],
      "metadata": {
        "id": "lE5dHhGA4PVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3faea112-b1f9-4fd2-f673-948c72eebda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agent\n",
        "model = PPO(\n",
        "    'MlpPolicy',\n",
        "    env,\n",
        "    verbose=1,\n",
        "    learning_rate=0.001,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    clip_range_vf=None,\n",
        "    ent_coef=0.01,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        "    use_sde=False,\n",
        "    sde_sample_freq=-1,\n",
        "    target_kl=None,\n",
        "    tensorboard_log=None,\n",
        "    create_eval_env=False,\n",
        "    policy_kwargs=None,\n",
        "    seed=None,\n",
        "    device='auto',\n",
        "    _init_setup_model=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsRxTFRiw1Ct",
        "outputId": "066f73ee-8544-434c-c155-f9d0b545525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO is not very sensitive to hyperparemeters. We have tried different hyperparameter setting but the results did not vary significantly."
      ],
      "metadata": {
        "id": "bisgfF5_OwPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add callbacks for early stopping and checkpointing\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./models/',\n",
        "                             log_path='./logs/', eval_freq=500,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./checkpoints/',\n",
        "                                         name_prefix='mountain')"
      ],
      "metadata": {
        "id": "qVXn5Jd9LDVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train agent\n",
        "model.learn(total_timesteps=400000, callback=[eval_callback, checkpoint_callback])\n",
        "\n",
        "# Save trained agent\n",
        "model.save(\"ppo_ev_control_400000\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4wV0TM3A3FC",
        "outputId": "1e27d891-5df7-4819-f601-cc31e68834a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    mean_reward          | 132         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 430         |\n",
            "|    ep_rew_mean          | 124         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 110         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 852         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006795411 |\n",
            "|    clip_fraction        | 0.0486      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.14        |\n",
            "|    explained_variance   | 0.93        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.76        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00106    |\n",
            "|    std                  | 0.209       |\n",
            "|    value_loss           | 18.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=94500, episode_reward=133.53 +/- 5.91\n",
            "Episode length: 547.80 +/- 121.13\n",
            "Eval num_timesteps=95000, episode_reward=130.96 +/- 4.21\n",
            "Episode length: 503.40 +/- 67.67\n",
            "Eval num_timesteps=95500, episode_reward=120.95 +/- 35.34\n",
            "Episode length: 720.60 +/- 189.21\n",
            "Eval num_timesteps=96000, episode_reward=132.16 +/- 9.42\n",
            "Episode length: 538.60 +/- 193.78\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 539          |\n",
            "|    mean_reward          | 132          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 432          |\n",
            "|    ep_rew_mean          | 124          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 111          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 866          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016806521 |\n",
            "|    clip_fraction        | 0.028        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.141        |\n",
            "|    explained_variance   | 0.875        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.73         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.000144    |\n",
            "|    std                  | 0.212        |\n",
            "|    value_loss           | 21.2         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=96500, episode_reward=135.14 +/- 5.60\n",
            "Episode length: 591.00 +/- 111.73\n",
            "Eval num_timesteps=97000, episode_reward=132.36 +/- 3.49\n",
            "Episode length: 515.60 +/- 71.95\n",
            "Eval num_timesteps=97500, episode_reward=138.21 +/- 5.17\n",
            "Episode length: 648.20 +/- 111.55\n",
            "Eval num_timesteps=98000, episode_reward=131.72 +/- 4.37\n",
            "Episode length: 522.20 +/- 93.10\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 522           |\n",
            "|    mean_reward          | 132           |\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 431           |\n",
            "|    ep_rew_mean          | 124           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 111           |\n",
            "|    iterations           | 48            |\n",
            "|    time_elapsed         | 880           |\n",
            "|    total_timesteps      | 98304         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | -0.0015573057 |\n",
            "|    clip_fraction        | 0.0254        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | 0.135         |\n",
            "|    explained_variance   | 0.929         |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 16.5          |\n",
            "|    n_updates            | 470           |\n",
            "|    policy_gradient_loss | -0.000807     |\n",
            "|    std                  | 0.21          |\n",
            "|    value_loss           | 23.1          |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=98500, episode_reward=128.87 +/- 4.04\n",
            "Episode length: 467.20 +/- 93.33\n",
            "Eval num_timesteps=99000, episode_reward=132.57 +/- 11.70\n",
            "Episode length: 547.40 +/- 220.70\n",
            "Eval num_timesteps=99500, episode_reward=114.23 +/- 31.46\n",
            "Episode length: 594.20 +/- 211.37\n",
            "Eval num_timesteps=100000, episode_reward=129.50 +/- 1.66\n",
            "Episode length: 474.40 +/- 29.57\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 474         |\n",
            "|    mean_reward          | 129         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 431         |\n",
            "|    ep_rew_mean          | 123         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 112         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 893         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010802964 |\n",
            "|    clip_fraction        | 0.0375      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.135       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.5        |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.00134    |\n",
            "|    std                  | 0.213       |\n",
            "|    value_loss           | 25.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=100500, episode_reward=128.92 +/- 3.97\n",
            "Episode length: 486.60 +/- 77.43\n",
            "Eval num_timesteps=101000, episode_reward=135.37 +/- 7.11\n",
            "Episode length: 615.40 +/- 148.74\n",
            "Eval num_timesteps=101500, episode_reward=128.16 +/- 5.07\n",
            "Episode length: 464.60 +/- 109.68\n",
            "Eval num_timesteps=102000, episode_reward=135.57 +/- 12.29\n",
            "Episode length: 618.60 +/- 242.75\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 619          |\n",
            "|    mean_reward          | 136          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 432          |\n",
            "|    ep_rew_mean          | 124          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 112          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 906          |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012047695 |\n",
            "|    clip_fraction        | 0.0608       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.122        |\n",
            "|    explained_variance   | 0.927        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.08         |\n",
            "|    n_updates            | 490          |\n",
            "|    policy_gradient_loss | 0.00166      |\n",
            "|    std                  | 0.215        |\n",
            "|    value_loss           | 16.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=102500, episode_reward=130.72 +/- 3.22\n",
            "Episode length: 505.60 +/- 57.92\n",
            "Eval num_timesteps=103000, episode_reward=126.81 +/- 3.28\n",
            "Episode length: 437.40 +/- 61.53\n",
            "Eval num_timesteps=103500, episode_reward=132.35 +/- 9.88\n",
            "Episode length: 543.20 +/- 194.32\n",
            "Eval num_timesteps=104000, episode_reward=130.66 +/- 5.10\n",
            "Episode length: 511.40 +/- 102.07\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 511        |\n",
            "|    mean_reward          | 131        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 431        |\n",
            "|    ep_rew_mean          | 124        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 113        |\n",
            "|    iterations           | 51         |\n",
            "|    time_elapsed         | 919        |\n",
            "|    total_timesteps      | 104448     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01293103 |\n",
            "|    clip_fraction        | 0.11       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.127      |\n",
            "|    explained_variance   | 0.808      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 4.22       |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | 0.00128    |\n",
            "|    std                  | 0.213      |\n",
            "|    value_loss           | 23.4       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=104500, episode_reward=127.64 +/- 8.53\n",
            "Episode length: 471.20 +/- 157.04\n",
            "Eval num_timesteps=105000, episode_reward=124.73 +/- 4.41\n",
            "Episode length: 401.80 +/- 98.46\n",
            "Eval num_timesteps=105500, episode_reward=128.81 +/- 4.20\n",
            "Episode length: 477.60 +/- 91.65\n",
            "Eval num_timesteps=106000, episode_reward=124.77 +/- 4.24\n",
            "Episode length: 405.80 +/- 81.29\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 406         |\n",
            "|    mean_reward          | 125         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 432         |\n",
            "|    ep_rew_mean          | 124         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 114         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 930         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012250826 |\n",
            "|    clip_fraction        | 0.0532      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.136       |\n",
            "|    explained_variance   | 0.742       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.1        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.000717   |\n",
            "|    std                  | 0.21        |\n",
            "|    value_loss           | 33.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=106500, episode_reward=132.13 +/- 7.29\n",
            "Episode length: 535.80 +/- 155.21\n",
            "Eval num_timesteps=107000, episode_reward=130.94 +/- 7.36\n",
            "Episode length: 516.20 +/- 159.19\n",
            "Eval num_timesteps=107500, episode_reward=115.38 +/- 33.32\n",
            "Episode length: 643.60 +/- 212.17\n",
            "Eval num_timesteps=108000, episode_reward=129.94 +/- 5.76\n",
            "Episode length: 505.80 +/- 119.27\n",
            "Eval num_timesteps=108500, episode_reward=131.35 +/- 8.72\n",
            "Episode length: 523.20 +/- 177.09\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 523       |\n",
            "|    mean_reward          | 131       |\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 431       |\n",
            "|    ep_rew_mean          | 123       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 114       |\n",
            "|    iterations           | 53        |\n",
            "|    time_elapsed         | 946       |\n",
            "|    total_timesteps      | 108544    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0079257 |\n",
            "|    clip_fraction        | 0.0439    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | 0.161     |\n",
            "|    explained_variance   | 0.899     |\n",
            "|    learning_rate        | 0.001     |\n",
            "|    loss                 | 1.38      |\n",
            "|    n_updates            | 520       |\n",
            "|    policy_gradient_loss | 0.00209   |\n",
            "|    std                  | 0.204     |\n",
            "|    value_loss           | 6.24      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=109000, episode_reward=130.99 +/- 3.94\n",
            "Episode length: 514.80 +/- 81.41\n",
            "Eval num_timesteps=109500, episode_reward=128.19 +/- 3.94\n",
            "Episode length: 464.60 +/- 60.08\n",
            "Eval num_timesteps=110000, episode_reward=125.80 +/- 4.02\n",
            "Episode length: 413.40 +/- 92.19\n",
            "Eval num_timesteps=110500, episode_reward=126.51 +/- 3.75\n",
            "Episode length: 436.00 +/- 65.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 436         |\n",
            "|    mean_reward          | 127         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 430         |\n",
            "|    ep_rew_mean          | 123         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 115         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 958         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006255441 |\n",
            "|    clip_fraction        | 0.0425      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.177       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5           |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | 0.000612    |\n",
            "|    std                  | 0.202       |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=111000, episode_reward=128.48 +/- 8.19\n",
            "Episode length: 476.40 +/- 157.36\n",
            "Eval num_timesteps=111500, episode_reward=127.88 +/- 4.88\n",
            "Episode length: 470.80 +/- 106.68\n",
            "Eval num_timesteps=112000, episode_reward=130.80 +/- 6.82\n",
            "Episode length: 518.60 +/- 145.17\n",
            "Eval num_timesteps=112500, episode_reward=130.74 +/- 5.54\n",
            "Episode length: 518.60 +/- 115.33\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 519          |\n",
            "|    mean_reward          | 131          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 430          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 116          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 971          |\n",
            "|    total_timesteps      | 112640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037705265 |\n",
            "|    clip_fraction        | 0.0548       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.169        |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 5.47         |\n",
            "|    n_updates            | 540          |\n",
            "|    policy_gradient_loss | 0.00184      |\n",
            "|    std                  | 0.206        |\n",
            "|    value_loss           | 8.77         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=113000, episode_reward=133.04 +/- 9.53\n",
            "Episode length: 565.40 +/- 204.00\n",
            "Eval num_timesteps=113500, episode_reward=129.65 +/- 4.55\n",
            "Episode length: 486.80 +/- 80.38\n",
            "Eval num_timesteps=114000, episode_reward=123.68 +/- 1.58\n",
            "Episode length: 372.00 +/- 26.47\n",
            "Eval num_timesteps=114500, episode_reward=130.54 +/- 7.53\n",
            "Episode length: 518.60 +/- 140.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 519         |\n",
            "|    mean_reward          | 131         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 430         |\n",
            "|    ep_rew_mean          | 123         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 116         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 983         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002657262 |\n",
            "|    clip_fraction        | 0.0297      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.156       |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.6        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | 9.54e-05    |\n",
            "|    std                  | 0.206       |\n",
            "|    value_loss           | 26.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=115000, episode_reward=128.52 +/- 8.10\n",
            "Episode length: 495.80 +/- 156.91\n",
            "Eval num_timesteps=115500, episode_reward=127.36 +/- 7.49\n",
            "Episode length: 468.20 +/- 158.79\n",
            "Eval num_timesteps=116000, episode_reward=123.06 +/- 3.54\n",
            "Episode length: 377.00 +/- 56.03\n",
            "Eval num_timesteps=116500, episode_reward=122.84 +/- 2.67\n",
            "Episode length: 386.00 +/- 47.51\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 386        |\n",
            "|    mean_reward          | 123        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 425        |\n",
            "|    ep_rew_mean          | 123        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 117        |\n",
            "|    iterations           | 57         |\n",
            "|    time_elapsed         | 995        |\n",
            "|    total_timesteps      | 116736     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00846717 |\n",
            "|    clip_fraction        | 0.11       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.182      |\n",
            "|    explained_variance   | 0.97       |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 1.11       |\n",
            "|    n_updates            | 560        |\n",
            "|    policy_gradient_loss | -7.28e-05  |\n",
            "|    std                  | 0.199      |\n",
            "|    value_loss           | 3.56       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=117000, episode_reward=127.33 +/- 5.33\n",
            "Episode length: 459.60 +/- 93.12\n",
            "Eval num_timesteps=117500, episode_reward=127.32 +/- 4.17\n",
            "Episode length: 460.80 +/- 81.68\n",
            "Eval num_timesteps=118000, episode_reward=126.08 +/- 4.09\n",
            "Episode length: 432.60 +/- 74.55\n",
            "Eval num_timesteps=118500, episode_reward=126.66 +/- 4.15\n",
            "Episode length: 458.20 +/- 78.93\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 458          |\n",
            "|    mean_reward          | 127          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 421          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 117          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 1006         |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054598385 |\n",
            "|    clip_fraction        | 0.037        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.179        |\n",
            "|    explained_variance   | 0.946        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 1.75         |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | 0.000923     |\n",
            "|    std                  | 0.205        |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=119000, episode_reward=125.74 +/- 12.17\n",
            "Episode length: 442.60 +/- 236.27\n",
            "Eval num_timesteps=119500, episode_reward=125.99 +/- 9.53\n",
            "Episode length: 461.20 +/- 187.84\n",
            "Eval num_timesteps=120000, episode_reward=126.43 +/- 6.10\n",
            "Episode length: 450.00 +/- 115.98\n",
            "Eval num_timesteps=120500, episode_reward=127.31 +/- 2.92\n",
            "Episode length: 479.80 +/- 61.05\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 480          |\n",
            "|    mean_reward          | 127          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 418          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 118          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 1018         |\n",
            "|    total_timesteps      | 120832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051336666 |\n",
            "|    clip_fraction        | 0.0601       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.166        |\n",
            "|    explained_variance   | 0.93         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 17.4         |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | 0.000567     |\n",
            "|    std                  | 0.203        |\n",
            "|    value_loss           | 25.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=121000, episode_reward=119.29 +/- 1.66\n",
            "Episode length: 318.60 +/- 46.86\n",
            "Eval num_timesteps=121500, episode_reward=126.75 +/- 7.94\n",
            "Episode length: 468.20 +/- 171.85\n",
            "Eval num_timesteps=122000, episode_reward=125.39 +/- 2.57\n",
            "Episode length: 452.40 +/- 47.59\n",
            "Eval num_timesteps=122500, episode_reward=119.66 +/- 1.45\n",
            "Episode length: 320.00 +/- 29.34\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 320         |\n",
            "|    mean_reward          | 120         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 414         |\n",
            "|    ep_rew_mean          | 123         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 1029        |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005644019 |\n",
            "|    clip_fraction        | 0.0623      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.168       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 15.1        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | 0.000122    |\n",
            "|    std                  | 0.205       |\n",
            "|    value_loss           | 19          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=123000, episode_reward=124.16 +/- 7.44\n",
            "Episode length: 430.40 +/- 147.42\n",
            "Eval num_timesteps=123500, episode_reward=124.81 +/- 4.31\n",
            "Episode length: 439.00 +/- 102.08\n",
            "Eval num_timesteps=124000, episode_reward=119.21 +/- 1.72\n",
            "Episode length: 332.00 +/- 20.41\n",
            "Eval num_timesteps=124500, episode_reward=119.74 +/- 2.56\n",
            "Episode length: 319.60 +/- 51.13\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 320          |\n",
            "|    mean_reward          | 120          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 412          |\n",
            "|    ep_rew_mean          | 122          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 120          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 1040         |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035998286 |\n",
            "|    clip_fraction        | 0.0579       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.183        |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.39         |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | 0.000346     |\n",
            "|    std                  | 0.197        |\n",
            "|    value_loss           | 6.7          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=125000, episode_reward=121.71 +/- 5.13\n",
            "Episode length: 395.80 +/- 94.91\n",
            "Eval num_timesteps=125500, episode_reward=119.70 +/- 2.81\n",
            "Episode length: 353.40 +/- 61.50\n",
            "Eval num_timesteps=126000, episode_reward=127.36 +/- 11.90\n",
            "Episode length: 485.60 +/- 232.10\n",
            "Eval num_timesteps=126500, episode_reward=118.44 +/- 2.90\n",
            "Episode length: 323.80 +/- 61.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 324         |\n",
            "|    mean_reward          | 118         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 409         |\n",
            "|    ep_rew_mean          | 122         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1050        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014832767 |\n",
            "|    clip_fraction        | 0.0617      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.213       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.54        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | 1.22e-05    |\n",
            "|    std                  | 0.196       |\n",
            "|    value_loss           | 7.33        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=127000, episode_reward=129.97 +/- 11.38\n",
            "Episode length: 545.20 +/- 214.00\n",
            "Eval num_timesteps=127500, episode_reward=122.86 +/- 3.77\n",
            "Episode length: 409.60 +/- 71.52\n",
            "Eval num_timesteps=128000, episode_reward=119.66 +/- 4.12\n",
            "Episode length: 343.80 +/- 68.72\n",
            "Eval num_timesteps=128500, episode_reward=123.62 +/- 2.48\n",
            "Episode length: 421.80 +/- 61.21\n",
            "Eval num_timesteps=129000, episode_reward=122.06 +/- 4.78\n",
            "Episode length: 383.20 +/- 89.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 383         |\n",
            "|    mean_reward          | 122         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 406         |\n",
            "|    ep_rew_mean          | 122         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 121         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1064        |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003992428 |\n",
            "|    clip_fraction        | 0.0767      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.207       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.01        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.000591   |\n",
            "|    std                  | 0.197       |\n",
            "|    value_loss           | 5.06        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=129500, episode_reward=122.67 +/- 5.75\n",
            "Episode length: 409.20 +/- 102.08\n",
            "Eval num_timesteps=130000, episode_reward=121.77 +/- 5.63\n",
            "Episode length: 387.60 +/- 94.64\n",
            "Eval num_timesteps=130500, episode_reward=120.04 +/- 2.91\n",
            "Episode length: 348.60 +/- 61.35\n",
            "Eval num_timesteps=131000, episode_reward=119.02 +/- 1.60\n",
            "Episode length: 327.60 +/- 37.06\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 328          |\n",
            "|    mean_reward          | 119          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 403          |\n",
            "|    ep_rew_mean          | 122          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 121          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 1074         |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013915522 |\n",
            "|    clip_fraction        | 0.0409       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.208        |\n",
            "|    explained_variance   | 0.987        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.78         |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | 0.00175      |\n",
            "|    std                  | 0.197        |\n",
            "|    value_loss           | 5.92         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=131500, episode_reward=123.94 +/- 2.46\n",
            "Episode length: 432.40 +/- 37.26\n",
            "Eval num_timesteps=132000, episode_reward=122.43 +/- 3.64\n",
            "Episode length: 394.00 +/- 64.75\n",
            "Eval num_timesteps=132500, episode_reward=119.71 +/- 3.00\n",
            "Episode length: 357.00 +/- 53.98\n",
            "Eval num_timesteps=133000, episode_reward=118.15 +/- 3.02\n",
            "Episode length: 336.00 +/- 55.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 336         |\n",
            "|    mean_reward          | 118         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 401         |\n",
            "|    ep_rew_mean          | 122         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1085        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011221236 |\n",
            "|    clip_fraction        | 0.0686      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.232       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 4.48        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.000444   |\n",
            "|    std                  | 0.188       |\n",
            "|    value_loss           | 7.94        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=133500, episode_reward=120.15 +/- 5.52\n",
            "Episode length: 355.80 +/- 109.49\n",
            "Eval num_timesteps=134000, episode_reward=120.78 +/- 4.75\n",
            "Episode length: 387.00 +/- 100.57\n",
            "Eval num_timesteps=134500, episode_reward=117.82 +/- 2.07\n",
            "Episode length: 329.40 +/- 26.93\n",
            "Eval num_timesteps=135000, episode_reward=123.31 +/- 5.81\n",
            "Episode length: 432.40 +/- 109.18\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 432         |\n",
            "|    mean_reward          | 123         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 397         |\n",
            "|    ep_rew_mean          | 121         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1095        |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018999835 |\n",
            "|    clip_fraction        | 0.0805      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.252       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 0.756       |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | 0.00232     |\n",
            "|    std                  | 0.188       |\n",
            "|    value_loss           | 4.24        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=135500, episode_reward=114.35 +/- 2.60\n",
            "Episode length: 275.20 +/- 61.96\n",
            "Eval num_timesteps=136000, episode_reward=115.84 +/- 2.70\n",
            "Episode length: 285.20 +/- 47.55\n",
            "Eval num_timesteps=136500, episode_reward=120.41 +/- 2.46\n",
            "Episode length: 378.40 +/- 70.38\n",
            "Eval num_timesteps=137000, episode_reward=118.87 +/- 6.88\n",
            "Episode length: 324.60 +/- 122.74\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 325         |\n",
            "|    mean_reward          | 119         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 392         |\n",
            "|    ep_rew_mean          | 121         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 1105        |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012570128 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.258       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.48        |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.000356   |\n",
            "|    std                  | 0.184       |\n",
            "|    value_loss           | 7.97        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=137500, episode_reward=115.80 +/- 4.09\n",
            "Episode length: 292.60 +/- 65.24\n",
            "Eval num_timesteps=138000, episode_reward=123.23 +/- 6.82\n",
            "Episode length: 434.20 +/- 143.00\n",
            "Eval num_timesteps=138500, episode_reward=117.18 +/- 2.07\n",
            "Episode length: 305.20 +/- 47.45\n",
            "Eval num_timesteps=139000, episode_reward=115.67 +/- 2.66\n",
            "Episode length: 289.60 +/- 54.48\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 290         |\n",
            "|    mean_reward          | 116         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 389         |\n",
            "|    ep_rew_mean          | 121         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1114        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005551081 |\n",
            "|    clip_fraction        | 0.0858      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.29        |\n",
            "|    explained_variance   | 0.956       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 65.4        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.000617   |\n",
            "|    std                  | 0.179       |\n",
            "|    value_loss           | 18.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=139500, episode_reward=117.96 +/- 4.46\n",
            "Episode length: 339.80 +/- 103.91\n",
            "Eval num_timesteps=140000, episode_reward=123.59 +/- 6.05\n",
            "Episode length: 430.40 +/- 108.87\n",
            "Eval num_timesteps=140500, episode_reward=122.43 +/- 5.48\n",
            "Episode length: 422.80 +/- 125.34\n",
            "Eval num_timesteps=141000, episode_reward=115.59 +/- 3.11\n",
            "Episode length: 283.20 +/- 59.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 283         |\n",
            "|    mean_reward          | 116         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 383         |\n",
            "|    ep_rew_mean          | 120         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1125        |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009242674 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.276       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.12        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00303    |\n",
            "|    std                  | 0.186       |\n",
            "|    value_loss           | 8.87        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=141500, episode_reward=117.09 +/- 4.13\n",
            "Episode length: 309.80 +/- 101.58\n",
            "Eval num_timesteps=142000, episode_reward=119.82 +/- 4.56\n",
            "Episode length: 358.40 +/- 96.38\n",
            "Eval num_timesteps=142500, episode_reward=118.59 +/- 7.92\n",
            "Episode length: 352.20 +/- 157.84\n",
            "Eval num_timesteps=143000, episode_reward=117.50 +/- 3.57\n",
            "Episode length: 309.40 +/- 63.61\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 309         |\n",
            "|    mean_reward          | 118         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 379         |\n",
            "|    ep_rew_mean          | 120         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1134        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002196303 |\n",
            "|    clip_fraction        | 0.0842      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.284       |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1           |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | 0.00249     |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 2.63        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=143500, episode_reward=121.39 +/- 8.11\n",
            "Episode length: 386.80 +/- 157.93\n",
            "Eval num_timesteps=144000, episode_reward=113.44 +/- 2.10\n",
            "Episode length: 242.60 +/- 28.39\n",
            "Eval num_timesteps=144500, episode_reward=114.43 +/- 2.50\n",
            "Episode length: 265.00 +/- 52.14\n",
            "Eval num_timesteps=145000, episode_reward=117.49 +/- 3.01\n",
            "Episode length: 320.60 +/- 69.20\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 321          |\n",
            "|    mean_reward          | 117          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 373          |\n",
            "|    ep_rew_mean          | 120          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 127          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 1144         |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041892813 |\n",
            "|    clip_fraction        | 0.0927       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.28         |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 3.24         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | 4.04e-05     |\n",
            "|    std                  | 0.186        |\n",
            "|    value_loss           | 10.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=145500, episode_reward=118.47 +/- 5.72\n",
            "Episode length: 337.60 +/- 104.26\n",
            "Eval num_timesteps=146000, episode_reward=115.53 +/- 3.45\n",
            "Episode length: 306.60 +/- 66.11\n",
            "Eval num_timesteps=146500, episode_reward=114.06 +/- 2.52\n",
            "Episode length: 280.40 +/- 40.25\n",
            "Eval num_timesteps=147000, episode_reward=112.60 +/- 1.71\n",
            "Episode length: 245.00 +/- 20.07\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 245          |\n",
            "|    mean_reward          | 113          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 369          |\n",
            "|    ep_rew_mean          | 119          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 127          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 1153         |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039578504 |\n",
            "|    clip_fraction        | 0.0667       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.263        |\n",
            "|    explained_variance   | 0.941        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 3.94         |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | 0.000709     |\n",
            "|    std                  | 0.187        |\n",
            "|    value_loss           | 22.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=147500, episode_reward=115.91 +/- 5.97\n",
            "Episode length: 334.20 +/- 104.57\n",
            "Eval num_timesteps=148000, episode_reward=111.35 +/- 1.65\n",
            "Episode length: 216.20 +/- 8.03\n",
            "Eval num_timesteps=148500, episode_reward=114.18 +/- 3.34\n",
            "Episode length: 279.00 +/- 60.75\n",
            "Eval num_timesteps=149000, episode_reward=112.68 +/- 0.63\n",
            "Episode length: 233.20 +/- 34.33\n",
            "Eval num_timesteps=149500, episode_reward=115.02 +/- 4.70\n",
            "Episode length: 286.40 +/- 107.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 286          |\n",
            "|    mean_reward          | 115          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 353          |\n",
            "|    ep_rew_mean          | 118          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 128          |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 1163         |\n",
            "|    total_timesteps      | 149504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040240176 |\n",
            "|    clip_fraction        | 0.0749       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.258        |\n",
            "|    explained_variance   | 0.911        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 59.7         |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.00129     |\n",
            "|    std                  | 0.188        |\n",
            "|    value_loss           | 38.5         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=117.72 +/- 3.86\n",
            "Episode length: 346.00 +/- 67.97\n",
            "Eval num_timesteps=150500, episode_reward=114.48 +/- 3.82\n",
            "Episode length: 269.20 +/- 55.46\n",
            "Eval num_timesteps=151000, episode_reward=118.60 +/- 7.13\n",
            "Episode length: 345.00 +/- 134.16\n",
            "Eval num_timesteps=151500, episode_reward=115.03 +/- 3.39\n",
            "Episode length: 291.60 +/- 73.07\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 292          |\n",
            "|    mean_reward          | 115          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 338          |\n",
            "|    ep_rew_mean          | 117          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 129          |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 1172         |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055087805 |\n",
            "|    clip_fraction        | 0.121        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.246        |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.65         |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | 0.00378      |\n",
            "|    std                  | 0.191        |\n",
            "|    value_loss           | 3.28         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=152000, episode_reward=113.81 +/- 2.27\n",
            "Episode length: 257.00 +/- 37.73\n",
            "Eval num_timesteps=152500, episode_reward=115.70 +/- 5.68\n",
            "Episode length: 339.60 +/- 117.92\n",
            "Eval num_timesteps=153000, episode_reward=113.83 +/- 1.51\n",
            "Episode length: 299.60 +/- 33.61\n",
            "Eval num_timesteps=153500, episode_reward=113.31 +/- 5.47\n",
            "Episode length: 267.40 +/- 79.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 267         |\n",
            "|    mean_reward          | 113         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 324         |\n",
            "|    ep_rew_mean          | 116         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 129         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1181        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008864749 |\n",
            "|    clip_fraction        | 0.0887      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.249       |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 32.3        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00214    |\n",
            "|    std                  | 0.189       |\n",
            "|    value_loss           | 25.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=154000, episode_reward=112.35 +/- 2.15\n",
            "Episode length: 229.00 +/- 27.06\n",
            "Eval num_timesteps=154500, episode_reward=113.82 +/- 4.49\n",
            "Episode length: 296.00 +/- 64.78\n",
            "Eval num_timesteps=155000, episode_reward=116.22 +/- 3.79\n",
            "Episode length: 337.80 +/- 58.56\n",
            "Eval num_timesteps=155500, episode_reward=115.06 +/- 4.08\n",
            "Episode length: 304.60 +/- 58.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 305          |\n",
            "|    mean_reward          | 115          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 313          |\n",
            "|    ep_rew_mean          | 116          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 130          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 1190         |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037377868 |\n",
            "|    clip_fraction        | 0.0458       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.259        |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 9.36         |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    std                  | 0.184        |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=156000, episode_reward=116.68 +/- 8.13\n",
            "Episode length: 343.20 +/- 154.10\n",
            "Eval num_timesteps=156500, episode_reward=109.97 +/- 1.22\n",
            "Episode length: 221.80 +/- 4.83\n",
            "Eval num_timesteps=157000, episode_reward=114.91 +/- 4.38\n",
            "Episode length: 304.20 +/- 66.89\n",
            "Eval num_timesteps=157500, episode_reward=113.26 +/- 3.28\n",
            "Episode length: 277.00 +/- 69.44\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 277         |\n",
            "|    mean_reward          | 113         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 305         |\n",
            "|    ep_rew_mean          | 115         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 131         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1199        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018730871 |\n",
            "|    clip_fraction        | 0.0665      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.279       |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.29        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.00095     |\n",
            "|    std                  | 0.183       |\n",
            "|    value_loss           | 28          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=158000, episode_reward=110.87 +/- 0.46\n",
            "Episode length: 224.60 +/- 48.48\n",
            "Eval num_timesteps=158500, episode_reward=109.70 +/- 1.69\n",
            "Episode length: 220.60 +/- 43.13\n",
            "Eval num_timesteps=159000, episode_reward=112.49 +/- 2.29\n",
            "Episode length: 244.00 +/- 55.10\n",
            "Eval num_timesteps=159500, episode_reward=110.02 +/- 1.98\n",
            "Episode length: 213.40 +/- 25.41\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 213          |\n",
            "|    mean_reward          | 110          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 291          |\n",
            "|    ep_rew_mean          | 114          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 132          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 1207         |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047598565 |\n",
            "|    clip_fraction        | 0.0681       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.295        |\n",
            "|    explained_variance   | 0.94         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 9.71         |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    std                  | 0.179        |\n",
            "|    value_loss           | 29.7         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=113.94 +/- 4.29\n",
            "Episode length: 281.40 +/- 112.10\n",
            "Eval num_timesteps=160500, episode_reward=112.53 +/- 2.58\n",
            "Episode length: 240.80 +/- 65.10\n",
            "Eval num_timesteps=161000, episode_reward=110.87 +/- 2.37\n",
            "Episode length: 225.40 +/- 24.75\n",
            "Eval num_timesteps=161500, episode_reward=113.27 +/- 2.70\n",
            "Episode length: 275.00 +/- 64.29\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 275          |\n",
            "|    mean_reward          | 113          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 283          |\n",
            "|    ep_rew_mean          | 114          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 133          |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 1216         |\n",
            "|    total_timesteps      | 161792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013538215 |\n",
            "|    clip_fraction        | 0.0766       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.313        |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 11.9         |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | 0.00335      |\n",
            "|    std                  | 0.177        |\n",
            "|    value_loss           | 11.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=162000, episode_reward=114.42 +/- 2.61\n",
            "Episode length: 326.40 +/- 67.25\n",
            "Eval num_timesteps=162500, episode_reward=111.52 +/- 1.95\n",
            "Episode length: 221.00 +/- 46.16\n",
            "Eval num_timesteps=163000, episode_reward=110.14 +/- 0.69\n",
            "Episode length: 180.80 +/- 22.11\n",
            "Eval num_timesteps=163500, episode_reward=112.99 +/- 3.17\n",
            "Episode length: 282.80 +/- 68.72\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 283         |\n",
            "|    mean_reward          | 113         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 275         |\n",
            "|    ep_rew_mean          | 113         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 133         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1224        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005347834 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.283       |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 4.88        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.000941   |\n",
            "|    std                  | 0.186       |\n",
            "|    value_loss           | 21.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=164000, episode_reward=112.07 +/- 2.70\n",
            "Episode length: 249.00 +/- 28.31\n",
            "Eval num_timesteps=164500, episode_reward=114.31 +/- 3.10\n",
            "Episode length: 258.20 +/- 63.16\n",
            "Eval num_timesteps=165000, episode_reward=112.30 +/- 2.36\n",
            "Episode length: 240.60 +/- 57.53\n",
            "Eval num_timesteps=165500, episode_reward=109.69 +/- 1.31\n",
            "Episode length: 217.80 +/- 22.80\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 218          |\n",
            "|    mean_reward          | 110          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 268          |\n",
            "|    ep_rew_mean          | 113          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 134          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 1232         |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047205267 |\n",
            "|    clip_fraction        | 0.0986       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.268        |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 0.59         |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00476      |\n",
            "|    std                  | 0.182        |\n",
            "|    value_loss           | 4.18         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=166000, episode_reward=110.21 +/- 2.66\n",
            "Episode length: 211.20 +/- 26.81\n",
            "Eval num_timesteps=166500, episode_reward=109.80 +/- 1.57\n",
            "Episode length: 203.60 +/- 15.21\n",
            "Eval num_timesteps=167000, episode_reward=111.22 +/- 1.76\n",
            "Episode length: 224.60 +/- 40.42\n",
            "Eval num_timesteps=167500, episode_reward=114.28 +/- 2.71\n",
            "Episode length: 287.80 +/- 60.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 288         |\n",
            "|    mean_reward          | 114         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 262         |\n",
            "|    ep_rew_mean          | 112         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 135         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1240        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008136602 |\n",
            "|    clip_fraction        | 0.0994      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.301       |\n",
            "|    explained_variance   | 0.951       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.54        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | 0.00333     |\n",
            "|    std                  | 0.176       |\n",
            "|    value_loss           | 20.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=168000, episode_reward=113.66 +/- 2.19\n",
            "Episode length: 253.20 +/- 43.02\n",
            "Eval num_timesteps=168500, episode_reward=110.54 +/- 2.15\n",
            "Episode length: 237.80 +/- 49.17\n",
            "Eval num_timesteps=169000, episode_reward=110.50 +/- 1.15\n",
            "Episode length: 203.80 +/- 31.91\n",
            "Eval num_timesteps=169500, episode_reward=109.16 +/- 1.77\n",
            "Episode length: 203.60 +/- 34.56\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 204          |\n",
            "|    mean_reward          | 109          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 254          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 136          |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 1248         |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073105157 |\n",
            "|    clip_fraction        | 0.111        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.324        |\n",
            "|    explained_variance   | 0.942        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 49.2         |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | 0.000261     |\n",
            "|    std                  | 0.175        |\n",
            "|    value_loss           | 29.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=109.88 +/- 1.22\n",
            "Episode length: 219.20 +/- 50.01\n",
            "Eval num_timesteps=170500, episode_reward=112.25 +/- 1.32\n",
            "Episode length: 251.60 +/- 57.15\n",
            "Eval num_timesteps=171000, episode_reward=112.31 +/- 2.59\n",
            "Episode length: 268.00 +/- 57.43\n",
            "Eval num_timesteps=171500, episode_reward=112.35 +/- 1.86\n",
            "Episode length: 231.40 +/- 63.01\n",
            "Eval num_timesteps=172000, episode_reward=110.75 +/- 0.97\n",
            "Episode length: 202.80 +/- 26.76\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 203          |\n",
            "|    mean_reward          | 111          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 111          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 136          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 1257         |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041616065 |\n",
            "|    clip_fraction        | 0.0627       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.317        |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 6.16         |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | 0.00188      |\n",
            "|    std                  | 0.178        |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=172500, episode_reward=109.29 +/- 2.99\n",
            "Episode length: 239.80 +/- 72.52\n",
            "Eval num_timesteps=173000, episode_reward=109.52 +/- 1.96\n",
            "Episode length: 226.00 +/- 56.00\n",
            "Eval num_timesteps=173500, episode_reward=108.36 +/- 1.97\n",
            "Episode length: 196.40 +/- 58.99\n",
            "Eval num_timesteps=174000, episode_reward=110.41 +/- 0.84\n",
            "Episode length: 179.40 +/- 10.76\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 179         |\n",
            "|    mean_reward          | 110         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 238         |\n",
            "|    ep_rew_mean          | 111         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 137         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1265        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002441456 |\n",
            "|    clip_fraction        | 0.0783      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.301       |\n",
            "|    explained_variance   | 0.944       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.92        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | -0.000623   |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 27.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=174500, episode_reward=110.44 +/- 3.29\n",
            "Episode length: 234.20 +/- 51.70\n",
            "Eval num_timesteps=175000, episode_reward=110.95 +/- 2.19\n",
            "Episode length: 282.60 +/- 45.59\n",
            "Eval num_timesteps=175500, episode_reward=108.94 +/- 1.51\n",
            "Episode length: 197.40 +/- 27.65\n",
            "Eval num_timesteps=176000, episode_reward=110.63 +/- 4.95\n",
            "Episode length: 226.60 +/- 88.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 227         |\n",
            "|    mean_reward          | 111         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 233         |\n",
            "|    ep_rew_mean          | 110         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 138         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1273        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006598221 |\n",
            "|    clip_fraction        | 0.0934      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.287       |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 11.8        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | 0.00137     |\n",
            "|    std                  | 0.181       |\n",
            "|    value_loss           | 30.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=176500, episode_reward=110.32 +/- 3.33\n",
            "Episode length: 239.00 +/- 79.03\n",
            "Eval num_timesteps=177000, episode_reward=109.13 +/- 2.47\n",
            "Episode length: 216.20 +/- 49.25\n",
            "Eval num_timesteps=177500, episode_reward=114.85 +/- 5.36\n",
            "Episode length: 337.60 +/- 86.08\n",
            "Eval num_timesteps=178000, episode_reward=107.02 +/- 1.36\n",
            "Episode length: 187.00 +/- 35.55\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 187        |\n",
            "|    mean_reward          | 107        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 227        |\n",
            "|    ep_rew_mean          | 110        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 138        |\n",
            "|    iterations           | 87         |\n",
            "|    time_elapsed         | 1281       |\n",
            "|    total_timesteps      | 178176     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00675404 |\n",
            "|    clip_fraction        | 0.0833     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.29       |\n",
            "|    explained_variance   | 0.962      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 4.93       |\n",
            "|    n_updates            | 860        |\n",
            "|    policy_gradient_loss | 0.00191    |\n",
            "|    std                  | 0.181      |\n",
            "|    value_loss           | 14.3       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=178500, episode_reward=113.69 +/- 3.46\n",
            "Episode length: 293.60 +/- 75.74\n",
            "Eval num_timesteps=179000, episode_reward=109.76 +/- 3.01\n",
            "Episode length: 223.20 +/- 69.81\n",
            "Eval num_timesteps=179500, episode_reward=112.12 +/- 3.19\n",
            "Episode length: 248.40 +/- 67.88\n",
            "Eval num_timesteps=180000, episode_reward=114.15 +/- 4.89\n",
            "Episode length: 269.40 +/- 74.36\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 269         |\n",
            "|    mean_reward          | 114         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | 110         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 139         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1290        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004854494 |\n",
            "|    clip_fraction        | 0.0936      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.284       |\n",
            "|    explained_variance   | 0.958       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.11        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | 0.000252    |\n",
            "|    std                  | 0.184       |\n",
            "|    value_loss           | 19          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=180500, episode_reward=110.37 +/- 1.90\n",
            "Episode length: 211.00 +/- 52.29\n",
            "Eval num_timesteps=181000, episode_reward=113.05 +/- 3.39\n",
            "Episode length: 263.80 +/- 45.26\n",
            "Eval num_timesteps=181500, episode_reward=111.28 +/- 2.09\n",
            "Episode length: 208.40 +/- 11.89\n",
            "Eval num_timesteps=182000, episode_reward=112.34 +/- 4.66\n",
            "Episode length: 266.00 +/- 72.67\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 266         |\n",
            "|    mean_reward          | 112         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 221         |\n",
            "|    ep_rew_mean          | 110         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 140         |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 1298        |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004340794 |\n",
            "|    clip_fraction        | 0.073       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.241       |\n",
            "|    explained_variance   | 0.965       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.28        |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.000977   |\n",
            "|    std                  | 0.193       |\n",
            "|    value_loss           | 15.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=182500, episode_reward=110.96 +/- 1.57\n",
            "Episode length: 217.80 +/- 42.76\n",
            "Eval num_timesteps=183000, episode_reward=109.63 +/- 2.19\n",
            "Episode length: 197.40 +/- 28.35\n",
            "Eval num_timesteps=183500, episode_reward=109.34 +/- 1.97\n",
            "Episode length: 176.60 +/- 27.62\n",
            "Eval num_timesteps=184000, episode_reward=108.89 +/- 2.93\n",
            "Episode length: 186.60 +/- 45.57\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 187         |\n",
            "|    mean_reward          | 109         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 220         |\n",
            "|    ep_rew_mean          | 110         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 141         |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 1305        |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018543322 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.224       |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 25          |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | 0.00341     |\n",
            "|    std                  | 0.194       |\n",
            "|    value_loss           | 33          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=184500, episode_reward=109.37 +/- 2.43\n",
            "Episode length: 218.80 +/- 58.96\n",
            "Eval num_timesteps=185000, episode_reward=111.92 +/- 5.03\n",
            "Episode length: 271.80 +/- 103.48\n",
            "Eval num_timesteps=185500, episode_reward=110.59 +/- 4.60\n",
            "Episode length: 243.40 +/- 70.33\n",
            "Eval num_timesteps=186000, episode_reward=112.09 +/- 3.02\n",
            "Episode length: 268.00 +/- 70.84\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 268          |\n",
            "|    mean_reward          | 112          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 109          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 141          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1314         |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0125405975 |\n",
            "|    clip_fraction        | 0.0985       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.207        |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.91         |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | 0.00374      |\n",
            "|    std                  | 0.199        |\n",
            "|    value_loss           | 6.73         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=186500, episode_reward=108.74 +/- 1.45\n",
            "Episode length: 251.20 +/- 42.12\n",
            "Eval num_timesteps=187000, episode_reward=107.09 +/- 1.44\n",
            "Episode length: 169.00 +/- 40.51\n",
            "Eval num_timesteps=187500, episode_reward=109.05 +/- 2.30\n",
            "Episode length: 210.80 +/- 54.12\n",
            "Eval num_timesteps=188000, episode_reward=108.93 +/- 2.81\n",
            "Episode length: 200.20 +/- 43.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 200         |\n",
            "|    mean_reward          | 109         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | 109         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 142         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1321        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006210402 |\n",
            "|    clip_fraction        | 0.076       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.196       |\n",
            "|    explained_variance   | 0.928       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 15.5        |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | 0.00162     |\n",
            "|    std                  | 0.2         |\n",
            "|    value_loss           | 37.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=188500, episode_reward=106.95 +/- 3.06\n",
            "Episode length: 174.60 +/- 42.19\n",
            "Eval num_timesteps=189000, episode_reward=104.28 +/- 0.81\n",
            "Episode length: 174.60 +/- 39.78\n",
            "Eval num_timesteps=189500, episode_reward=108.30 +/- 5.21\n",
            "Episode length: 203.60 +/- 84.00\n",
            "Eval num_timesteps=190000, episode_reward=106.34 +/- 1.46\n",
            "Episode length: 201.20 +/- 51.15\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 201          |\n",
            "|    mean_reward          | 106          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 109          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 143          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 1329         |\n",
            "|    total_timesteps      | 190464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071939724 |\n",
            "|    clip_fraction        | 0.0993       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.18         |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 5.49         |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -0.000939    |\n",
            "|    std                  | 0.205        |\n",
            "|    value_loss           | 18.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=190500, episode_reward=107.89 +/- 2.02\n",
            "Episode length: 195.40 +/- 45.89\n",
            "Eval num_timesteps=191000, episode_reward=109.61 +/- 3.18\n",
            "Episode length: 224.60 +/- 53.13\n",
            "Eval num_timesteps=191500, episode_reward=106.71 +/- 1.77\n",
            "Episode length: 187.60 +/- 40.25\n",
            "Eval num_timesteps=192000, episode_reward=108.99 +/- 3.84\n",
            "Episode length: 195.00 +/- 49.85\n",
            "Eval num_timesteps=192500, episode_reward=107.29 +/- 1.10\n",
            "Episode length: 180.60 +/- 36.85\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 181          |\n",
            "|    mean_reward          | 107          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 109          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 143          |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 1337         |\n",
            "|    total_timesteps      | 192512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030005844 |\n",
            "|    clip_fraction        | 0.0684       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.15         |\n",
            "|    explained_variance   | 0.947        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 2.8          |\n",
            "|    n_updates            | 930          |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    std                  | 0.211        |\n",
            "|    value_loss           | 27.2         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=193000, episode_reward=107.69 +/- 2.65\n",
            "Episode length: 202.20 +/- 53.86\n",
            "Eval num_timesteps=193500, episode_reward=108.78 +/- 2.56\n",
            "Episode length: 197.60 +/- 80.43\n",
            "Eval num_timesteps=194000, episode_reward=105.36 +/- 1.55\n",
            "Episode length: 151.40 +/- 13.09\n",
            "Eval num_timesteps=194500, episode_reward=106.16 +/- 1.70\n",
            "Episode length: 159.00 +/- 15.34\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 159         |\n",
            "|    mean_reward          | 106         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 207         |\n",
            "|    ep_rew_mean          | 108         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 144         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1344        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003927377 |\n",
            "|    clip_fraction        | 0.0548      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.132       |\n",
            "|    explained_variance   | 0.965       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 15.8        |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.000478   |\n",
            "|    std                  | 0.212       |\n",
            "|    value_loss           | 17.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=195000, episode_reward=106.09 +/- 1.37\n",
            "Episode length: 171.00 +/- 25.46\n",
            "Eval num_timesteps=195500, episode_reward=107.09 +/- 2.81\n",
            "Episode length: 200.20 +/- 36.22\n",
            "Eval num_timesteps=196000, episode_reward=108.99 +/- 5.55\n",
            "Episode length: 202.60 +/- 74.84\n",
            "Eval num_timesteps=196500, episode_reward=106.61 +/- 1.94\n",
            "Episode length: 172.60 +/- 25.97\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 173         |\n",
            "|    mean_reward          | 107         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 204         |\n",
            "|    ep_rew_mean          | 108         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 145         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 1351        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002443218 |\n",
            "|    clip_fraction        | 0.0802      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.129       |\n",
            "|    explained_variance   | 0.945       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 13          |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.000733   |\n",
            "|    std                  | 0.214       |\n",
            "|    value_loss           | 27.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=197000, episode_reward=108.73 +/- 5.37\n",
            "Episode length: 202.00 +/- 67.89\n",
            "Eval num_timesteps=197500, episode_reward=105.84 +/- 2.29\n",
            "Episode length: 166.00 +/- 28.61\n",
            "Eval num_timesteps=198000, episode_reward=106.40 +/- 2.95\n",
            "Episode length: 180.20 +/- 33.40\n",
            "Eval num_timesteps=198500, episode_reward=110.13 +/- 7.49\n",
            "Episode length: 239.40 +/- 115.89\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 239          |\n",
            "|    mean_reward          | 110          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 108          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 146          |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 1359         |\n",
            "|    total_timesteps      | 198656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046725105 |\n",
            "|    clip_fraction        | 0.0677       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.13         |\n",
            "|    explained_variance   | 0.946        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 8.35         |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | 0.00138      |\n",
            "|    std                  | 0.211        |\n",
            "|    value_loss           | 23.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=199000, episode_reward=106.47 +/- 2.53\n",
            "Episode length: 179.20 +/- 32.62\n",
            "Eval num_timesteps=199500, episode_reward=107.57 +/- 2.65\n",
            "Episode length: 181.20 +/- 37.37\n",
            "Eval num_timesteps=200000, episode_reward=106.65 +/- 2.06\n",
            "Episode length: 175.80 +/- 15.74\n",
            "Eval num_timesteps=200500, episode_reward=108.48 +/- 3.72\n",
            "Episode length: 187.60 +/- 42.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 188         |\n",
            "|    mean_reward          | 108         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 198         |\n",
            "|    ep_rew_mean          | 108         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 146         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1366        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008921906 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.153       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.08        |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | 0.0022      |\n",
            "|    std                  | 0.207       |\n",
            "|    value_loss           | 6.6         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=201000, episode_reward=105.92 +/- 1.95\n",
            "Episode length: 154.20 +/- 26.39\n",
            "Eval num_timesteps=201500, episode_reward=109.50 +/- 2.70\n",
            "Episode length: 229.20 +/- 43.30\n",
            "Eval num_timesteps=202000, episode_reward=106.02 +/- 1.98\n",
            "Episode length: 193.80 +/- 57.66\n",
            "Eval num_timesteps=202500, episode_reward=103.45 +/- 1.34\n",
            "Episode length: 155.20 +/- 30.44\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 155         |\n",
            "|    mean_reward          | 103         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 195         |\n",
            "|    ep_rew_mean          | 107         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 147         |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 1373        |\n",
            "|    total_timesteps      | 202752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009699373 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.169       |\n",
            "|    explained_variance   | 0.956       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.28        |\n",
            "|    n_updates            | 980         |\n",
            "|    policy_gradient_loss | 0.00218     |\n",
            "|    std                  | 0.205       |\n",
            "|    value_loss           | 15          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=203000, episode_reward=106.14 +/- 2.30\n",
            "Episode length: 173.00 +/- 27.41\n",
            "Eval num_timesteps=203500, episode_reward=108.66 +/- 2.25\n",
            "Episode length: 169.60 +/- 27.85\n",
            "Eval num_timesteps=204000, episode_reward=106.00 +/- 1.05\n",
            "Episode length: 152.40 +/- 32.00\n",
            "Eval num_timesteps=204500, episode_reward=105.41 +/- 2.55\n",
            "Episode length: 156.00 +/- 18.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 156         |\n",
            "|    mean_reward          | 105         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 193         |\n",
            "|    ep_rew_mean          | 107         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 148         |\n",
            "|    iterations           | 100         |\n",
            "|    time_elapsed         | 1380        |\n",
            "|    total_timesteps      | 204800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008453373 |\n",
            "|    clip_fraction        | 0.0685      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.17        |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 14.4        |\n",
            "|    n_updates            | 990         |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    std                  | 0.204       |\n",
            "|    value_loss           | 25          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=205000, episode_reward=106.56 +/- 1.85\n",
            "Episode length: 165.60 +/- 40.27\n",
            "Eval num_timesteps=205500, episode_reward=106.84 +/- 2.27\n",
            "Episode length: 166.40 +/- 16.28\n",
            "Eval num_timesteps=206000, episode_reward=108.71 +/- 5.02\n",
            "Episode length: 196.60 +/- 67.68\n",
            "Eval num_timesteps=206500, episode_reward=106.69 +/- 2.35\n",
            "Episode length: 158.20 +/- 35.45\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 158          |\n",
            "|    mean_reward          | 107          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 187          |\n",
            "|    ep_rew_mean          | 107          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 149          |\n",
            "|    iterations           | 101          |\n",
            "|    time_elapsed         | 1387         |\n",
            "|    total_timesteps      | 206848       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076037757 |\n",
            "|    clip_fraction        | 0.0893       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.186        |\n",
            "|    explained_variance   | 0.918        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 5.82         |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | 0.00209      |\n",
            "|    std                  | 0.2          |\n",
            "|    value_loss           | 40.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=207000, episode_reward=106.35 +/- 1.22\n",
            "Episode length: 161.20 +/- 26.60\n",
            "Eval num_timesteps=207500, episode_reward=105.92 +/- 1.10\n",
            "Episode length: 186.80 +/- 27.16\n",
            "Eval num_timesteps=208000, episode_reward=105.53 +/- 3.16\n",
            "Episode length: 184.40 +/- 42.01\n",
            "Eval num_timesteps=208500, episode_reward=106.33 +/- 1.24\n",
            "Episode length: 166.80 +/- 32.57\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 167         |\n",
            "|    mean_reward          | 106         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 182         |\n",
            "|    ep_rew_mean          | 106         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 149         |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 1394        |\n",
            "|    total_timesteps      | 208896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007798861 |\n",
            "|    clip_fraction        | 0.0732      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.174       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 13.4        |\n",
            "|    n_updates            | 1010        |\n",
            "|    policy_gradient_loss | -0.00176    |\n",
            "|    std                  | 0.207       |\n",
            "|    value_loss           | 53.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=209000, episode_reward=106.02 +/- 2.39\n",
            "Episode length: 176.00 +/- 36.43\n",
            "Eval num_timesteps=209500, episode_reward=105.10 +/- 3.33\n",
            "Episode length: 150.20 +/- 15.28\n",
            "Eval num_timesteps=210000, episode_reward=106.16 +/- 1.78\n",
            "Episode length: 176.60 +/- 21.90\n",
            "Eval num_timesteps=210500, episode_reward=106.08 +/- 1.88\n",
            "Episode length: 179.60 +/- 47.38\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 180        |\n",
            "|    mean_reward          | 106        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 180        |\n",
            "|    ep_rew_mean          | 106        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 150        |\n",
            "|    iterations           | 103        |\n",
            "|    time_elapsed         | 1401       |\n",
            "|    total_timesteps      | 210944     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01548555 |\n",
            "|    clip_fraction        | 0.146      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.158      |\n",
            "|    explained_variance   | 0.979      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 3.3        |\n",
            "|    n_updates            | 1020       |\n",
            "|    policy_gradient_loss | 0.00482    |\n",
            "|    std                  | 0.206      |\n",
            "|    value_loss           | 8.03       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=211000, episode_reward=104.12 +/- 1.75\n",
            "Episode length: 162.00 +/- 27.62\n",
            "Eval num_timesteps=211500, episode_reward=104.56 +/- 1.30\n",
            "Episode length: 156.20 +/- 27.07\n",
            "Eval num_timesteps=212000, episode_reward=103.30 +/- 1.16\n",
            "Episode length: 148.00 +/- 17.50\n",
            "Eval num_timesteps=212500, episode_reward=104.39 +/- 2.49\n",
            "Episode length: 169.80 +/- 56.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 170         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 174         |\n",
            "|    ep_rew_mean          | 106         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 151         |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 1407        |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010466469 |\n",
            "|    clip_fraction        | 0.0972      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.148       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 4.9         |\n",
            "|    n_updates            | 1030        |\n",
            "|    policy_gradient_loss | 0.000877    |\n",
            "|    std                  | 0.211       |\n",
            "|    value_loss           | 34.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=213000, episode_reward=103.60 +/- 1.98\n",
            "Episode length: 158.80 +/- 21.13\n",
            "Eval num_timesteps=213500, episode_reward=105.88 +/- 5.23\n",
            "Episode length: 184.40 +/- 86.86\n",
            "Eval num_timesteps=214000, episode_reward=106.37 +/- 2.29\n",
            "Episode length: 159.40 +/- 16.70\n",
            "Eval num_timesteps=214500, episode_reward=106.26 +/- 2.35\n",
            "Episode length: 171.20 +/- 12.77\n",
            "Eval num_timesteps=215000, episode_reward=104.55 +/- 0.69\n",
            "Episode length: 161.20 +/- 21.20\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 161         |\n",
            "|    mean_reward          | 105         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 174         |\n",
            "|    ep_rew_mean          | 106         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 151         |\n",
            "|    iterations           | 105         |\n",
            "|    time_elapsed         | 1415        |\n",
            "|    total_timesteps      | 215040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012197793 |\n",
            "|    clip_fraction        | 0.0928      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.124       |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 7.51        |\n",
            "|    n_updates            | 1040        |\n",
            "|    policy_gradient_loss | 0.000722    |\n",
            "|    std                  | 0.216       |\n",
            "|    value_loss           | 16.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=215500, episode_reward=104.41 +/- 3.33\n",
            "Episode length: 151.40 +/- 15.38\n",
            "Eval num_timesteps=216000, episode_reward=103.58 +/- 2.01\n",
            "Episode length: 149.20 +/- 13.23\n",
            "Eval num_timesteps=216500, episode_reward=104.60 +/- 3.51\n",
            "Episode length: 176.00 +/- 55.08\n",
            "Eval num_timesteps=217000, episode_reward=105.56 +/- 4.00\n",
            "Episode length: 172.00 +/- 27.46\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 172         |\n",
            "|    mean_reward          | 106         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 171         |\n",
            "|    ep_rew_mean          | 106         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 152         |\n",
            "|    iterations           | 106         |\n",
            "|    time_elapsed         | 1422        |\n",
            "|    total_timesteps      | 217088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004772472 |\n",
            "|    clip_fraction        | 0.0705      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.117       |\n",
            "|    explained_variance   | 0.956       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.96        |\n",
            "|    n_updates            | 1050        |\n",
            "|    policy_gradient_loss | -0.000588   |\n",
            "|    std                  | 0.216       |\n",
            "|    value_loss           | 22.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=217500, episode_reward=106.61 +/- 1.11\n",
            "Episode length: 172.80 +/- 38.62\n",
            "Eval num_timesteps=218000, episode_reward=106.90 +/- 1.39\n",
            "Episode length: 200.00 +/- 40.82\n",
            "Eval num_timesteps=218500, episode_reward=105.37 +/- 1.88\n",
            "Episode length: 167.00 +/- 42.28\n",
            "Eval num_timesteps=219000, episode_reward=104.07 +/- 1.94\n",
            "Episode length: 161.60 +/- 27.81\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 162        |\n",
            "|    mean_reward          | 104        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 167        |\n",
            "|    ep_rew_mean          | 105        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 153        |\n",
            "|    iterations           | 107        |\n",
            "|    time_elapsed         | 1429       |\n",
            "|    total_timesteps      | 219136     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01976811 |\n",
            "|    clip_fraction        | 0.107      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.111      |\n",
            "|    explained_variance   | 0.953      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 4.46       |\n",
            "|    n_updates            | 1060       |\n",
            "|    policy_gradient_loss | 0.0011     |\n",
            "|    std                  | 0.217      |\n",
            "|    value_loss           | 25.3       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=219500, episode_reward=106.02 +/- 1.25\n",
            "Episode length: 179.80 +/- 27.49\n",
            "Eval num_timesteps=220000, episode_reward=105.15 +/- 3.04\n",
            "Episode length: 159.20 +/- 22.12\n",
            "Eval num_timesteps=220500, episode_reward=104.47 +/- 2.01\n",
            "Episode length: 168.20 +/- 26.78\n",
            "Eval num_timesteps=221000, episode_reward=103.90 +/- 1.94\n",
            "Episode length: 150.00 +/- 13.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 150         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 164         |\n",
            "|    ep_rew_mean          | 105         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 153         |\n",
            "|    iterations           | 108         |\n",
            "|    time_elapsed         | 1436        |\n",
            "|    total_timesteps      | 221184      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027105272 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.111       |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.3         |\n",
            "|    n_updates            | 1070        |\n",
            "|    policy_gradient_loss | 0.00314     |\n",
            "|    std                  | 0.217       |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=221500, episode_reward=102.85 +/- 2.84\n",
            "Episode length: 170.60 +/- 59.72\n",
            "Eval num_timesteps=222000, episode_reward=103.73 +/- 2.28\n",
            "Episode length: 143.80 +/- 19.76\n",
            "Eval num_timesteps=222500, episode_reward=105.69 +/- 1.69\n",
            "Episode length: 154.40 +/- 38.64\n",
            "Eval num_timesteps=223000, episode_reward=105.17 +/- 3.00\n",
            "Episode length: 158.20 +/- 37.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 158         |\n",
            "|    mean_reward          | 105         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 162         |\n",
            "|    ep_rew_mean          | 105         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 154         |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 1443        |\n",
            "|    total_timesteps      | 223232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008827975 |\n",
            "|    clip_fraction        | 0.0864      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0986      |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 4.37        |\n",
            "|    n_updates            | 1080        |\n",
            "|    policy_gradient_loss | -0.00189    |\n",
            "|    std                  | 0.221       |\n",
            "|    value_loss           | 20.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=223500, episode_reward=106.20 +/- 3.07\n",
            "Episode length: 164.60 +/- 55.60\n",
            "Eval num_timesteps=224000, episode_reward=105.79 +/- 1.32\n",
            "Episode length: 141.60 +/- 24.80\n",
            "Eval num_timesteps=224500, episode_reward=102.98 +/- 1.74\n",
            "Episode length: 149.20 +/- 16.69\n",
            "Eval num_timesteps=225000, episode_reward=104.82 +/- 0.67\n",
            "Episode length: 167.80 +/- 26.78\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 168        |\n",
            "|    mean_reward          | 105        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 162        |\n",
            "|    ep_rew_mean          | 105        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 155        |\n",
            "|    iterations           | 110        |\n",
            "|    time_elapsed         | 1449       |\n",
            "|    total_timesteps      | 225280     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00998915 |\n",
            "|    clip_fraction        | 0.112      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.0861     |\n",
            "|    explained_variance   | 0.969      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 15.6       |\n",
            "|    n_updates            | 1090       |\n",
            "|    policy_gradient_loss | 0.0021     |\n",
            "|    std                  | 0.222      |\n",
            "|    value_loss           | 14.1       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=225500, episode_reward=104.05 +/- 2.81\n",
            "Episode length: 145.20 +/- 19.86\n",
            "Eval num_timesteps=226000, episode_reward=103.69 +/- 1.71\n",
            "Episode length: 151.40 +/- 33.15\n",
            "Eval num_timesteps=226500, episode_reward=103.36 +/- 1.84\n",
            "Episode length: 166.60 +/- 28.67\n",
            "Eval num_timesteps=227000, episode_reward=103.73 +/- 2.80\n",
            "Episode length: 152.40 +/- 15.55\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 152         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 157         |\n",
            "|    ep_rew_mean          | 105         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 111         |\n",
            "|    time_elapsed         | 1456        |\n",
            "|    total_timesteps      | 227328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010300297 |\n",
            "|    clip_fraction        | 0.0815      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0806      |\n",
            "|    explained_variance   | 0.942       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | 0.00416     |\n",
            "|    std                  | 0.221       |\n",
            "|    value_loss           | 31.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=227500, episode_reward=107.55 +/- 2.72\n",
            "Episode length: 178.00 +/- 44.66\n",
            "Eval num_timesteps=228000, episode_reward=103.43 +/- 1.69\n",
            "Episode length: 156.20 +/- 16.33\n",
            "Eval num_timesteps=228500, episode_reward=103.91 +/- 3.01\n",
            "Episode length: 138.80 +/- 26.53\n",
            "Eval num_timesteps=229000, episode_reward=103.18 +/- 3.24\n",
            "Episode length: 169.60 +/- 19.90\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 170         |\n",
            "|    mean_reward          | 103         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 155         |\n",
            "|    ep_rew_mean          | 105         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 156         |\n",
            "|    iterations           | 112         |\n",
            "|    time_elapsed         | 1463        |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015116183 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0855      |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.1        |\n",
            "|    n_updates            | 1110        |\n",
            "|    policy_gradient_loss | 0.00129     |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 41.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=229500, episode_reward=102.97 +/- 2.79\n",
            "Episode length: 160.00 +/- 19.26\n",
            "Eval num_timesteps=230000, episode_reward=103.50 +/- 2.17\n",
            "Episode length: 135.00 +/- 46.35\n",
            "Eval num_timesteps=230500, episode_reward=100.93 +/- 0.94\n",
            "Episode length: 145.40 +/- 6.12\n",
            "Eval num_timesteps=231000, episode_reward=102.22 +/- 1.78\n",
            "Episode length: 154.40 +/- 44.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 154         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 150         |\n",
            "|    ep_rew_mean          | 104         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 157         |\n",
            "|    iterations           | 113         |\n",
            "|    time_elapsed         | 1470        |\n",
            "|    total_timesteps      | 231424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008365324 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.102       |\n",
            "|    explained_variance   | 0.952       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.85        |\n",
            "|    n_updates            | 1120        |\n",
            "|    policy_gradient_loss | -0.00408    |\n",
            "|    std                  | 0.221       |\n",
            "|    value_loss           | 21.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=231500, episode_reward=101.66 +/- 2.32\n",
            "Episode length: 161.80 +/- 17.68\n",
            "Eval num_timesteps=232000, episode_reward=105.14 +/- 1.57\n",
            "Episode length: 141.00 +/- 37.64\n",
            "Eval num_timesteps=232500, episode_reward=103.06 +/- 2.48\n",
            "Episode length: 161.40 +/- 35.74\n",
            "Eval num_timesteps=233000, episode_reward=102.19 +/- 2.37\n",
            "Episode length: 118.60 +/- 32.20\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 119         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 147         |\n",
            "|    ep_rew_mean          | 104         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 158         |\n",
            "|    iterations           | 114         |\n",
            "|    time_elapsed         | 1476        |\n",
            "|    total_timesteps      | 233472      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010676779 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.108       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.53        |\n",
            "|    n_updates            | 1130        |\n",
            "|    policy_gradient_loss | 0.000326    |\n",
            "|    std                  | 0.216       |\n",
            "|    value_loss           | 19.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=233500, episode_reward=105.26 +/- 3.46\n",
            "Episode length: 175.20 +/- 38.71\n",
            "Eval num_timesteps=234000, episode_reward=103.16 +/- 2.55\n",
            "Episode length: 148.40 +/- 4.50\n",
            "Eval num_timesteps=234500, episode_reward=102.90 +/- 3.07\n",
            "Episode length: 132.40 +/- 12.88\n",
            "Eval num_timesteps=235000, episode_reward=101.29 +/- 0.98\n",
            "Episode length: 128.00 +/- 25.04\n",
            "Eval num_timesteps=235500, episode_reward=102.09 +/- 1.72\n",
            "Episode length: 145.20 +/- 2.71\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 145         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 146         |\n",
            "|    ep_rew_mean          | 104         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 158         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 1483        |\n",
            "|    total_timesteps      | 235520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010908706 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.1         |\n",
            "|    explained_variance   | 0.906       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 13          |\n",
            "|    n_updates            | 1140        |\n",
            "|    policy_gradient_loss | -0.000562   |\n",
            "|    std                  | 0.221       |\n",
            "|    value_loss           | 30.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=236000, episode_reward=103.35 +/- 1.59\n",
            "Episode length: 127.40 +/- 33.02\n",
            "Eval num_timesteps=236500, episode_reward=103.64 +/- 2.21\n",
            "Episode length: 124.20 +/- 13.38\n",
            "Eval num_timesteps=237000, episode_reward=102.96 +/- 1.30\n",
            "Episode length: 124.40 +/- 34.50\n",
            "Eval num_timesteps=237500, episode_reward=101.10 +/- 1.65\n",
            "Episode length: 144.40 +/- 17.48\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 144        |\n",
            "|    mean_reward          | 101        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 143        |\n",
            "|    ep_rew_mean          | 104        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 159        |\n",
            "|    iterations           | 116        |\n",
            "|    time_elapsed         | 1490       |\n",
            "|    total_timesteps      | 237568     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01701646 |\n",
            "|    clip_fraction        | 0.107      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.0783     |\n",
            "|    explained_variance   | 0.942      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 6.29       |\n",
            "|    n_updates            | 1150       |\n",
            "|    policy_gradient_loss | 0.0062     |\n",
            "|    std                  | 0.225      |\n",
            "|    value_loss           | 22.5       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=238000, episode_reward=105.52 +/- 3.69\n",
            "Episode length: 176.20 +/- 41.37\n",
            "Eval num_timesteps=238500, episode_reward=102.39 +/- 1.17\n",
            "Episode length: 125.20 +/- 40.24\n",
            "Eval num_timesteps=239000, episode_reward=102.89 +/- 0.49\n",
            "Episode length: 153.80 +/- 37.49\n",
            "Eval num_timesteps=239500, episode_reward=104.03 +/- 2.03\n",
            "Episode length: 140.20 +/- 41.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 140         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | 103         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 160         |\n",
            "|    iterations           | 117         |\n",
            "|    time_elapsed         | 1496        |\n",
            "|    total_timesteps      | 239616      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025918268 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0933      |\n",
            "|    explained_variance   | 0.93        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.17        |\n",
            "|    n_updates            | 1160        |\n",
            "|    policy_gradient_loss | 0.00235     |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 26.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=104.43 +/- 2.44\n",
            "Episode length: 130.40 +/- 35.47\n",
            "Eval num_timesteps=240500, episode_reward=103.19 +/- 1.47\n",
            "Episode length: 131.00 +/- 16.59\n",
            "Eval num_timesteps=241000, episode_reward=101.56 +/- 1.70\n",
            "Episode length: 135.00 +/- 12.85\n",
            "Eval num_timesteps=241500, episode_reward=102.33 +/- 1.25\n",
            "Episode length: 133.20 +/- 35.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 133         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | 103         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 160         |\n",
            "|    iterations           | 118         |\n",
            "|    time_elapsed         | 1502        |\n",
            "|    total_timesteps      | 241664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006588868 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0933      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.77        |\n",
            "|    n_updates            | 1170        |\n",
            "|    policy_gradient_loss | 0.00346     |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=242000, episode_reward=102.94 +/- 1.78\n",
            "Episode length: 148.00 +/- 48.85\n",
            "Eval num_timesteps=242500, episode_reward=103.05 +/- 1.54\n",
            "Episode length: 126.60 +/- 35.93\n",
            "Eval num_timesteps=243000, episode_reward=102.07 +/- 1.49\n",
            "Episode length: 132.60 +/- 50.04\n",
            "Eval num_timesteps=243500, episode_reward=101.37 +/- 1.46\n",
            "Episode length: 127.20 +/- 26.37\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 127          |\n",
            "|    mean_reward          | 101          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 142          |\n",
            "|    ep_rew_mean          | 103          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 161          |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 1509         |\n",
            "|    total_timesteps      | 243712       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0126958685 |\n",
            "|    clip_fraction        | 0.124        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0883       |\n",
            "|    explained_variance   | 0.953        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 10.7         |\n",
            "|    n_updates            | 1180         |\n",
            "|    policy_gradient_loss | 0.00314      |\n",
            "|    std                  | 0.225        |\n",
            "|    value_loss           | 15.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=244000, episode_reward=103.23 +/- 1.92\n",
            "Episode length: 124.80 +/- 13.42\n",
            "Eval num_timesteps=244500, episode_reward=102.99 +/- 0.96\n",
            "Episode length: 132.60 +/- 34.20\n",
            "Eval num_timesteps=245000, episode_reward=101.53 +/- 1.42\n",
            "Episode length: 139.00 +/- 14.85\n",
            "Eval num_timesteps=245500, episode_reward=103.72 +/- 1.89\n",
            "Episode length: 145.00 +/- 30.24\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 145        |\n",
            "|    mean_reward          | 104        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 141        |\n",
            "|    ep_rew_mean          | 103        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 162        |\n",
            "|    iterations           | 120        |\n",
            "|    time_elapsed         | 1515       |\n",
            "|    total_timesteps      | 245760     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01852772 |\n",
            "|    clip_fraction        | 0.162      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.061      |\n",
            "|    explained_variance   | 0.973      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 1.58       |\n",
            "|    n_updates            | 1190       |\n",
            "|    policy_gradient_loss | 0.00233    |\n",
            "|    std                  | 0.231      |\n",
            "|    value_loss           | 10.5       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=246000, episode_reward=102.08 +/- 1.27\n",
            "Episode length: 118.20 +/- 27.52\n",
            "Eval num_timesteps=246500, episode_reward=102.27 +/- 1.58\n",
            "Episode length: 132.00 +/- 16.09\n",
            "Eval num_timesteps=247000, episode_reward=104.10 +/- 2.16\n",
            "Episode length: 169.40 +/- 39.08\n",
            "Eval num_timesteps=247500, episode_reward=103.66 +/- 2.29\n",
            "Episode length: 129.00 +/- 35.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 129         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 141         |\n",
            "|    ep_rew_mean          | 103         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 162         |\n",
            "|    iterations           | 121         |\n",
            "|    time_elapsed         | 1522        |\n",
            "|    total_timesteps      | 247808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004190713 |\n",
            "|    clip_fraction        | 0.0723      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0244      |\n",
            "|    explained_variance   | 0.923       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.87        |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | 0.001       |\n",
            "|    std                  | 0.24        |\n",
            "|    value_loss           | 27.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=248000, episode_reward=102.27 +/- 2.26\n",
            "Episode length: 151.00 +/- 33.63\n",
            "Eval num_timesteps=248500, episode_reward=102.54 +/- 1.74\n",
            "Episode length: 132.40 +/- 34.31\n",
            "Eval num_timesteps=249000, episode_reward=102.32 +/- 1.62\n",
            "Episode length: 143.20 +/- 36.56\n",
            "Eval num_timesteps=249500, episode_reward=103.68 +/- 2.08\n",
            "Episode length: 120.20 +/- 36.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 120         |\n",
            "|    mean_reward          | 104         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | 103         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 163         |\n",
            "|    iterations           | 122         |\n",
            "|    time_elapsed         | 1528        |\n",
            "|    total_timesteps      | 249856      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013039278 |\n",
            "|    clip_fraction        | 0.0975      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.00849     |\n",
            "|    explained_variance   | 0.951       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.3        |\n",
            "|    n_updates            | 1210        |\n",
            "|    policy_gradient_loss | -0.00362    |\n",
            "|    std                  | 0.241       |\n",
            "|    value_loss           | 17.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=101.28 +/- 1.22\n",
            "Episode length: 118.00 +/- 23.89\n",
            "Eval num_timesteps=250500, episode_reward=102.86 +/- 0.64\n",
            "Episode length: 134.60 +/- 18.52\n",
            "Eval num_timesteps=251000, episode_reward=102.64 +/- 1.25\n",
            "Episode length: 146.00 +/- 49.55\n",
            "Eval num_timesteps=251500, episode_reward=103.17 +/- 1.65\n",
            "Episode length: 139.80 +/- 26.78\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 140         |\n",
            "|    mean_reward          | 103         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 138         |\n",
            "|    ep_rew_mean          | 103         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 123         |\n",
            "|    time_elapsed         | 1534        |\n",
            "|    total_timesteps      | 251904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009746561 |\n",
            "|    clip_fraction        | 0.0739      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.00348    |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 1220        |\n",
            "|    policy_gradient_loss | 0.00129     |\n",
            "|    std                  | 0.244       |\n",
            "|    value_loss           | 26.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=252000, episode_reward=102.75 +/- 3.11\n",
            "Episode length: 147.40 +/- 22.03\n",
            "Eval num_timesteps=252500, episode_reward=102.38 +/- 2.65\n",
            "Episode length: 140.00 +/- 47.45\n",
            "Eval num_timesteps=253000, episode_reward=103.03 +/- 2.56\n",
            "Episode length: 114.00 +/- 70.03\n",
            "Eval num_timesteps=253500, episode_reward=100.27 +/- 0.81\n",
            "Episode length: 136.80 +/- 15.64\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 137         |\n",
            "|    mean_reward          | 100         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 139         |\n",
            "|    ep_rew_mean          | 102         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 164         |\n",
            "|    iterations           | 124         |\n",
            "|    time_elapsed         | 1541        |\n",
            "|    total_timesteps      | 253952      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008953262 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.00289    |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 11.8        |\n",
            "|    n_updates            | 1230        |\n",
            "|    policy_gradient_loss | 4.35e-05    |\n",
            "|    std                  | 0.241       |\n",
            "|    value_loss           | 16.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=254000, episode_reward=102.10 +/- 2.06\n",
            "Episode length: 139.60 +/- 24.35\n",
            "Eval num_timesteps=254500, episode_reward=102.50 +/- 0.94\n",
            "Episode length: 105.40 +/- 38.00\n",
            "Eval num_timesteps=255000, episode_reward=101.11 +/- 1.39\n",
            "Episode length: 138.80 +/- 27.48\n",
            "Eval num_timesteps=255500, episode_reward=102.75 +/- 1.50\n",
            "Episode length: 136.20 +/- 45.71\n",
            "Eval num_timesteps=256000, episode_reward=101.44 +/- 0.84\n",
            "Episode length: 116.00 +/- 23.55\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 116          |\n",
            "|    mean_reward          | 101          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 102          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 165          |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 1547         |\n",
            "|    total_timesteps      | 256000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070745703 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0358       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 3.74         |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | 0.00299      |\n",
            "|    std                  | 0.23         |\n",
            "|    value_loss           | 7.51         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=256500, episode_reward=102.69 +/- 2.61\n",
            "Episode length: 144.00 +/- 37.16\n",
            "Eval num_timesteps=257000, episode_reward=102.64 +/- 2.24\n",
            "Episode length: 127.60 +/- 20.14\n",
            "Eval num_timesteps=257500, episode_reward=100.93 +/- 1.67\n",
            "Episode length: 144.20 +/- 22.36\n",
            "Eval num_timesteps=258000, episode_reward=100.17 +/- 1.47\n",
            "Episode length: 125.80 +/- 28.90\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 126        |\n",
            "|    mean_reward          | 100        |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 128        |\n",
            "|    ep_rew_mean          | 102        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 166        |\n",
            "|    iterations           | 126        |\n",
            "|    time_elapsed         | 1554       |\n",
            "|    total_timesteps      | 258048     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01592748 |\n",
            "|    clip_fraction        | 0.0906     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.0706     |\n",
            "|    explained_variance   | 0.909      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 28.1       |\n",
            "|    n_updates            | 1250       |\n",
            "|    policy_gradient_loss | -0.000228  |\n",
            "|    std                  | 0.223      |\n",
            "|    value_loss           | 39.9       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=258500, episode_reward=100.66 +/- 1.56\n",
            "Episode length: 138.20 +/- 13.20\n",
            "Eval num_timesteps=259000, episode_reward=101.76 +/- 2.62\n",
            "Episode length: 132.60 +/- 11.55\n",
            "Eval num_timesteps=259500, episode_reward=100.70 +/- 1.24\n",
            "Episode length: 115.00 +/- 34.79\n",
            "Eval num_timesteps=260000, episode_reward=100.51 +/- 1.02\n",
            "Episode length: 103.40 +/- 25.05\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 103          |\n",
            "|    mean_reward          | 101          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 102          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 166          |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 1560         |\n",
            "|    total_timesteps      | 260096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074278014 |\n",
            "|    clip_fraction        | 0.111        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0856       |\n",
            "|    explained_variance   | 0.859        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 27.3         |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -0.000112    |\n",
            "|    std                  | 0.221        |\n",
            "|    value_loss           | 57.5         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=260500, episode_reward=101.19 +/- 1.44\n",
            "Episode length: 119.20 +/- 38.87\n",
            "Eval num_timesteps=261000, episode_reward=102.74 +/- 2.34\n",
            "Episode length: 149.80 +/- 46.94\n",
            "Eval num_timesteps=261500, episode_reward=103.05 +/- 2.09\n",
            "Episode length: 111.40 +/- 27.90\n",
            "Eval num_timesteps=262000, episode_reward=99.37 +/- 0.38\n",
            "Episode length: 117.40 +/- 34.35\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 117          |\n",
            "|    mean_reward          | 99.4         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 101          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 167          |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 1566         |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049966257 |\n",
            "|    clip_fraction        | 0.088        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0968       |\n",
            "|    explained_variance   | 0.878        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 7.97         |\n",
            "|    n_updates            | 1270         |\n",
            "|    policy_gradient_loss | 0.000236     |\n",
            "|    std                  | 0.219        |\n",
            "|    value_loss           | 47           |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=262500, episode_reward=99.86 +/- 0.90\n",
            "Episode length: 103.00 +/- 31.87\n",
            "Eval num_timesteps=263000, episode_reward=102.06 +/- 2.11\n",
            "Episode length: 130.40 +/- 56.36\n",
            "Eval num_timesteps=263500, episode_reward=100.46 +/- 0.97\n",
            "Episode length: 109.80 +/- 39.53\n",
            "Eval num_timesteps=264000, episode_reward=101.38 +/- 1.58\n",
            "Episode length: 113.40 +/- 40.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 113         |\n",
            "|    mean_reward          | 101         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 129         |\n",
            "|    time_elapsed         | 1572        |\n",
            "|    total_timesteps      | 264192      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005494858 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.103       |\n",
            "|    explained_variance   | 0.944       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 31.9        |\n",
            "|    n_updates            | 1280        |\n",
            "|    policy_gradient_loss | 0.00368     |\n",
            "|    std                  | 0.22        |\n",
            "|    value_loss           | 19.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=264500, episode_reward=100.63 +/- 1.37\n",
            "Episode length: 123.60 +/- 36.49\n",
            "Eval num_timesteps=265000, episode_reward=100.76 +/- 1.78\n",
            "Episode length: 97.40 +/- 18.34\n",
            "Eval num_timesteps=265500, episode_reward=100.84 +/- 1.60\n",
            "Episode length: 141.20 +/- 25.45\n",
            "Eval num_timesteps=266000, episode_reward=102.08 +/- 1.83\n",
            "Episode length: 120.40 +/- 37.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 120         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 111         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 168         |\n",
            "|    iterations           | 130         |\n",
            "|    time_elapsed         | 1578        |\n",
            "|    total_timesteps      | 266240      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007886509 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0834      |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.31        |\n",
            "|    n_updates            | 1290        |\n",
            "|    policy_gradient_loss | 0.0081      |\n",
            "|    std                  | 0.222       |\n",
            "|    value_loss           | 8.45        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=266500, episode_reward=100.47 +/- 0.87\n",
            "Episode length: 102.80 +/- 25.58\n",
            "Eval num_timesteps=267000, episode_reward=100.80 +/- 1.94\n",
            "Episode length: 110.00 +/- 21.06\n",
            "Eval num_timesteps=267500, episode_reward=102.74 +/- 2.47\n",
            "Episode length: 107.60 +/- 16.86\n",
            "Eval num_timesteps=268000, episode_reward=100.88 +/- 1.57\n",
            "Episode length: 137.80 +/- 25.84\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 138         |\n",
            "|    mean_reward          | 101         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 112         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 131         |\n",
            "|    time_elapsed         | 1584        |\n",
            "|    total_timesteps      | 268288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015801612 |\n",
            "|    clip_fraction        | 0.0961      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0658      |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 4.93        |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | -0.000983   |\n",
            "|    std                  | 0.231       |\n",
            "|    value_loss           | 30.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=268500, episode_reward=101.13 +/- 1.38\n",
            "Episode length: 99.00 +/- 13.83\n",
            "Eval num_timesteps=269000, episode_reward=100.28 +/- 1.11\n",
            "Episode length: 111.40 +/- 31.50\n",
            "Eval num_timesteps=269500, episode_reward=100.48 +/- 2.47\n",
            "Episode length: 124.80 +/- 26.14\n",
            "Eval num_timesteps=270000, episode_reward=101.86 +/- 2.79\n",
            "Episode length: 147.60 +/- 59.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 148         |\n",
            "|    mean_reward          | 102         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 112         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 169         |\n",
            "|    iterations           | 132         |\n",
            "|    time_elapsed         | 1590        |\n",
            "|    total_timesteps      | 270336      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005569484 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0395      |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.9         |\n",
            "|    n_updates            | 1310        |\n",
            "|    policy_gradient_loss | 0.00381     |\n",
            "|    std                  | 0.234       |\n",
            "|    value_loss           | 8.44        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=270500, episode_reward=99.41 +/- 0.48\n",
            "Episode length: 100.20 +/- 24.46\n",
            "Eval num_timesteps=271000, episode_reward=98.92 +/- 0.29\n",
            "Episode length: 127.00 +/- 19.67\n",
            "Eval num_timesteps=271500, episode_reward=101.64 +/- 2.13\n",
            "Episode length: 111.20 +/- 25.10\n",
            "Eval num_timesteps=272000, episode_reward=99.48 +/- 0.90\n",
            "Episode length: 103.80 +/- 29.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 104         |\n",
            "|    mean_reward          | 99.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 113         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 170         |\n",
            "|    iterations           | 133         |\n",
            "|    time_elapsed         | 1596        |\n",
            "|    total_timesteps      | 272384      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010597476 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0345      |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.03        |\n",
            "|    n_updates            | 1320        |\n",
            "|    policy_gradient_loss | -0.00135    |\n",
            "|    std                  | 0.233       |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=272500, episode_reward=100.27 +/- 1.53\n",
            "Episode length: 131.80 +/- 32.18\n",
            "Eval num_timesteps=273000, episode_reward=99.57 +/- 0.61\n",
            "Episode length: 107.80 +/- 21.48\n",
            "Eval num_timesteps=273500, episode_reward=101.09 +/- 1.52\n",
            "Episode length: 115.60 +/- 46.38\n",
            "Eval num_timesteps=274000, episode_reward=100.97 +/- 2.24\n",
            "Episode length: 118.80 +/- 42.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 119         |\n",
            "|    mean_reward          | 101         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 171         |\n",
            "|    iterations           | 134         |\n",
            "|    time_elapsed         | 1602        |\n",
            "|    total_timesteps      | 274432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010838383 |\n",
            "|    clip_fraction        | 0.0924      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0304      |\n",
            "|    explained_variance   | 0.927       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 7.54        |\n",
            "|    n_updates            | 1330        |\n",
            "|    policy_gradient_loss | -0.0023     |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 26          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=274500, episode_reward=99.41 +/- 0.69\n",
            "Episode length: 128.80 +/- 16.38\n",
            "Eval num_timesteps=275000, episode_reward=101.02 +/- 1.73\n",
            "Episode length: 110.40 +/- 19.81\n",
            "Eval num_timesteps=275500, episode_reward=101.51 +/- 1.38\n",
            "Episode length: 118.80 +/- 36.60\n",
            "Eval num_timesteps=276000, episode_reward=100.02 +/- 0.88\n",
            "Episode length: 100.80 +/- 12.32\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 101          |\n",
            "|    mean_reward          | 100          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 111          |\n",
            "|    ep_rew_mean          | 101          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 171          |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 1608         |\n",
            "|    total_timesteps      | 276480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0104281865 |\n",
            "|    clip_fraction        | 0.124        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0297       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 4.03         |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | 0.00235      |\n",
            "|    std                  | 0.235        |\n",
            "|    value_loss           | 8.43         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=276500, episode_reward=99.86 +/- 1.37\n",
            "Episode length: 119.40 +/- 30.37\n",
            "Eval num_timesteps=277000, episode_reward=100.14 +/- 1.34\n",
            "Episode length: 120.00 +/- 43.90\n",
            "Eval num_timesteps=277500, episode_reward=99.34 +/- 1.01\n",
            "Episode length: 102.40 +/- 27.28\n",
            "Eval num_timesteps=278000, episode_reward=101.10 +/- 2.82\n",
            "Episode length: 126.20 +/- 16.63\n",
            "Eval num_timesteps=278500, episode_reward=99.44 +/- 1.05\n",
            "Episode length: 92.60 +/- 15.67\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 92.6         |\n",
            "|    mean_reward          | 99.4         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 101          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 172          |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 1615         |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014289404 |\n",
            "|    clip_fraction        | 0.0446       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0386       |\n",
            "|    explained_variance   | 0.864        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 36.1         |\n",
            "|    n_updates            | 1350         |\n",
            "|    policy_gradient_loss | -0.00194     |\n",
            "|    std                  | 0.231        |\n",
            "|    value_loss           | 46.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=279000, episode_reward=99.07 +/- 0.83\n",
            "Episode length: 102.40 +/- 27.94\n",
            "Eval num_timesteps=279500, episode_reward=99.97 +/- 1.63\n",
            "Episode length: 118.80 +/- 36.31\n",
            "Eval num_timesteps=280000, episode_reward=101.78 +/- 2.23\n",
            "Episode length: 114.20 +/- 21.63\n",
            "Eval num_timesteps=280500, episode_reward=101.43 +/- 1.62\n",
            "Episode length: 106.80 +/- 27.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 107         |\n",
            "|    mean_reward          | 101         |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 110         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 173         |\n",
            "|    iterations           | 137         |\n",
            "|    time_elapsed         | 1620        |\n",
            "|    total_timesteps      | 280576      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012676057 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.071       |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 24.6        |\n",
            "|    n_updates            | 1360        |\n",
            "|    policy_gradient_loss | 0.000609    |\n",
            "|    std                  | 0.223       |\n",
            "|    value_loss           | 29.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=281000, episode_reward=101.13 +/- 1.60\n",
            "Episode length: 107.20 +/- 39.67\n",
            "Eval num_timesteps=281500, episode_reward=99.11 +/- 0.81\n",
            "Episode length: 108.00 +/- 29.89\n",
            "Eval num_timesteps=282000, episode_reward=98.91 +/- 0.30\n",
            "Episode length: 113.60 +/- 25.90\n",
            "Eval num_timesteps=282500, episode_reward=99.12 +/- 0.77\n",
            "Episode length: 111.00 +/- 34.55\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 111          |\n",
            "|    mean_reward          | 99.1         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 107          |\n",
            "|    ep_rew_mean          | 100          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 173          |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 1626         |\n",
            "|    total_timesteps      | 282624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061432347 |\n",
            "|    clip_fraction        | 0.0822       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0682       |\n",
            "|    explained_variance   | 0.92         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 39.8         |\n",
            "|    n_updates            | 1370         |\n",
            "|    policy_gradient_loss | -0.00088     |\n",
            "|    std                  | 0.23         |\n",
            "|    value_loss           | 31.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=283000, episode_reward=100.27 +/- 1.46\n",
            "Episode length: 116.40 +/- 36.47\n",
            "Eval num_timesteps=283500, episode_reward=99.76 +/- 1.56\n",
            "Episode length: 88.40 +/- 16.93\n",
            "Eval num_timesteps=284000, episode_reward=99.87 +/- 0.94\n",
            "Episode length: 88.80 +/- 13.50\n",
            "Eval num_timesteps=284500, episode_reward=99.80 +/- 0.99\n",
            "Episode length: 95.80 +/- 15.59\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 95.8        |\n",
            "|    mean_reward          | 99.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 105         |\n",
            "|    ep_rew_mean          | 100         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 174         |\n",
            "|    iterations           | 139         |\n",
            "|    time_elapsed         | 1632        |\n",
            "|    total_timesteps      | 284672      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012265563 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0559      |\n",
            "|    explained_variance   | 0.928       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.63        |\n",
            "|    n_updates            | 1380        |\n",
            "|    policy_gradient_loss | 0.00437     |\n",
            "|    std                  | 0.229       |\n",
            "|    value_loss           | 26.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=285000, episode_reward=99.11 +/- 0.94\n",
            "Episode length: 117.80 +/- 28.67\n",
            "Eval num_timesteps=285500, episode_reward=100.79 +/- 1.66\n",
            "Episode length: 112.60 +/- 42.42\n",
            "Eval num_timesteps=286000, episode_reward=98.93 +/- 0.49\n",
            "Episode length: 107.20 +/- 23.89\n",
            "Eval num_timesteps=286500, episode_reward=99.32 +/- 1.64\n",
            "Episode length: 89.00 +/- 18.50\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 89           |\n",
            "|    mean_reward          | 99.3         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 106          |\n",
            "|    ep_rew_mean          | 100          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 174          |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 1638         |\n",
            "|    total_timesteps      | 286720       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029239985 |\n",
            "|    clip_fraction        | 0.0813       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0514       |\n",
            "|    explained_variance   | 0.869        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 6.94         |\n",
            "|    n_updates            | 1390         |\n",
            "|    policy_gradient_loss | 0.00322      |\n",
            "|    std                  | 0.233        |\n",
            "|    value_loss           | 37.5         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=287000, episode_reward=98.50 +/- 0.19\n",
            "Episode length: 85.60 +/- 14.22\n",
            "Eval num_timesteps=287500, episode_reward=100.31 +/- 2.18\n",
            "Episode length: 91.20 +/- 18.84\n",
            "Eval num_timesteps=288000, episode_reward=99.46 +/- 1.45\n",
            "Episode length: 82.20 +/- 10.68\n",
            "Eval num_timesteps=288500, episode_reward=99.03 +/- 0.62\n",
            "Episode length: 100.40 +/- 24.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 100         |\n",
            "|    mean_reward          | 99          |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 106         |\n",
            "|    ep_rew_mean          | 100         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 175         |\n",
            "|    iterations           | 141         |\n",
            "|    time_elapsed         | 1644        |\n",
            "|    total_timesteps      | 288768      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012710843 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0275      |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.49        |\n",
            "|    n_updates            | 1400        |\n",
            "|    policy_gradient_loss | 0.00262     |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 6.46        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=289000, episode_reward=100.44 +/- 1.18\n",
            "Episode length: 90.20 +/- 15.88\n",
            "Eval num_timesteps=289500, episode_reward=100.08 +/- 1.44\n",
            "Episode length: 102.40 +/- 22.78\n",
            "Eval num_timesteps=290000, episode_reward=99.78 +/- 0.85\n",
            "Episode length: 101.80 +/- 14.44\n",
            "Eval num_timesteps=290500, episode_reward=98.88 +/- 0.15\n",
            "Episode length: 107.40 +/- 31.65\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 107         |\n",
            "|    mean_reward          | 98.9        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 105         |\n",
            "|    ep_rew_mean          | 100         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 176         |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 1649        |\n",
            "|    total_timesteps      | 290816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007594114 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0266      |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.41        |\n",
            "|    n_updates            | 1410        |\n",
            "|    policy_gradient_loss | 0.00281     |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 12.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=291000, episode_reward=99.64 +/- 1.07\n",
            "Episode length: 111.00 +/- 32.13\n",
            "Eval num_timesteps=291500, episode_reward=99.46 +/- 0.97\n",
            "Episode length: 87.60 +/- 17.10\n",
            "Eval num_timesteps=292000, episode_reward=99.52 +/- 1.05\n",
            "Episode length: 104.00 +/- 29.91\n",
            "Eval num_timesteps=292500, episode_reward=99.76 +/- 1.22\n",
            "Episode length: 96.00 +/- 26.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 96           |\n",
            "|    mean_reward          | 99.8         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 105          |\n",
            "|    ep_rew_mean          | 100          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 176          |\n",
            "|    iterations           | 143          |\n",
            "|    time_elapsed         | 1655         |\n",
            "|    total_timesteps      | 292864       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022827284 |\n",
            "|    clip_fraction        | 0.0787       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0206       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 15           |\n",
            "|    n_updates            | 1420         |\n",
            "|    policy_gradient_loss | 0.00437      |\n",
            "|    std                  | 0.234        |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=293000, episode_reward=98.90 +/- 0.46\n",
            "Episode length: 91.20 +/- 14.47\n",
            "Eval num_timesteps=293500, episode_reward=98.93 +/- 0.35\n",
            "Episode length: 116.60 +/- 34.79\n",
            "Eval num_timesteps=294000, episode_reward=99.80 +/- 1.31\n",
            "Episode length: 96.20 +/- 18.81\n",
            "Eval num_timesteps=294500, episode_reward=100.03 +/- 1.14\n",
            "Episode length: 104.80 +/- 26.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 105          |\n",
            "|    mean_reward          | 100          |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 103          |\n",
            "|    ep_rew_mean          | 100          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 177          |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 1661         |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0137037765 |\n",
            "|    clip_fraction        | 0.156        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0405       |\n",
            "|    explained_variance   | 0.944        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 17.8         |\n",
            "|    n_updates            | 1430         |\n",
            "|    policy_gradient_loss | 0.0133       |\n",
            "|    std                  | 0.231        |\n",
            "|    value_loss           | 23.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=295000, episode_reward=100.12 +/- 1.84\n",
            "Episode length: 119.00 +/- 16.98\n",
            "Eval num_timesteps=295500, episode_reward=99.21 +/- 0.57\n",
            "Episode length: 101.80 +/- 26.22\n",
            "Eval num_timesteps=296000, episode_reward=98.80 +/- 0.46\n",
            "Episode length: 101.00 +/- 26.74\n",
            "Eval num_timesteps=296500, episode_reward=98.77 +/- 0.71\n",
            "Episode length: 82.00 +/- 15.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 82          |\n",
            "|    mean_reward          | 98.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 100         |\n",
            "|    ep_rew_mean          | 99.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 178         |\n",
            "|    iterations           | 145         |\n",
            "|    time_elapsed         | 1667        |\n",
            "|    total_timesteps      | 296960      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019254554 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0522      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.72        |\n",
            "|    n_updates            | 1440        |\n",
            "|    policy_gradient_loss | 0.00281     |\n",
            "|    std                  | 0.228       |\n",
            "|    value_loss           | 10.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=297000, episode_reward=98.29 +/- 0.37\n",
            "Episode length: 88.20 +/- 29.40\n",
            "Eval num_timesteps=297500, episode_reward=98.85 +/- 1.08\n",
            "Episode length: 88.20 +/- 15.30\n",
            "Eval num_timesteps=298000, episode_reward=98.34 +/- 0.56\n",
            "Episode length: 85.80 +/- 15.79\n",
            "Eval num_timesteps=298500, episode_reward=99.47 +/- 0.83\n",
            "Episode length: 94.20 +/- 10.38\n",
            "Eval num_timesteps=299000, episode_reward=98.69 +/- 0.10\n",
            "Episode length: 97.40 +/- 11.76\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 97.4        |\n",
            "|    mean_reward          | 98.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 99.9        |\n",
            "|    ep_rew_mean          | 100         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 178         |\n",
            "|    iterations           | 146         |\n",
            "|    time_elapsed         | 1672        |\n",
            "|    total_timesteps      | 299008      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010368504 |\n",
            "|    clip_fraction        | 0.0965      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0469      |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.09        |\n",
            "|    n_updates            | 1450        |\n",
            "|    policy_gradient_loss | 0.00378     |\n",
            "|    std                  | 0.232       |\n",
            "|    value_loss           | 13          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=299500, episode_reward=98.94 +/- 0.70\n",
            "Episode length: 82.40 +/- 13.89\n",
            "Eval num_timesteps=300000, episode_reward=98.61 +/- 0.27\n",
            "Episode length: 85.80 +/- 13.64\n",
            "Eval num_timesteps=300500, episode_reward=98.83 +/- 0.86\n",
            "Episode length: 92.40 +/- 13.54\n",
            "Eval num_timesteps=301000, episode_reward=99.47 +/- 0.68\n",
            "Episode length: 104.80 +/- 26.35\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 105          |\n",
            "|    mean_reward          | 99.5         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 97.7         |\n",
            "|    ep_rew_mean          | 99.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 179          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 1678         |\n",
            "|    total_timesteps      | 301056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068414295 |\n",
            "|    clip_fraction        | 0.128        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0516       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 0.844        |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | 0.00499      |\n",
            "|    std                  | 0.229        |\n",
            "|    value_loss           | 12.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=301500, episode_reward=99.28 +/- 1.38\n",
            "Episode length: 94.60 +/- 17.90\n",
            "Eval num_timesteps=302000, episode_reward=98.97 +/- 0.83\n",
            "Episode length: 110.20 +/- 27.53\n",
            "Eval num_timesteps=302500, episode_reward=98.33 +/- 0.60\n",
            "Episode length: 90.80 +/- 14.55\n",
            "Eval num_timesteps=303000, episode_reward=99.11 +/- 1.60\n",
            "Episode length: 103.60 +/- 29.42\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 104          |\n",
            "|    mean_reward          | 99.1         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 95.5         |\n",
            "|    ep_rew_mean          | 99.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 179          |\n",
            "|    iterations           | 148          |\n",
            "|    time_elapsed         | 1684         |\n",
            "|    total_timesteps      | 303104       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0080187535 |\n",
            "|    clip_fraction        | 0.105        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0594       |\n",
            "|    explained_variance   | 0.931        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 1.68         |\n",
            "|    n_updates            | 1470         |\n",
            "|    policy_gradient_loss | 0.00193      |\n",
            "|    std                  | 0.226        |\n",
            "|    value_loss           | 25.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=303500, episode_reward=98.28 +/- 0.76\n",
            "Episode length: 80.00 +/- 11.59\n",
            "Eval num_timesteps=304000, episode_reward=100.09 +/- 1.36\n",
            "Episode length: 102.40 +/- 30.06\n",
            "Eval num_timesteps=304500, episode_reward=98.96 +/- 0.19\n",
            "Episode length: 101.80 +/- 27.69\n",
            "Eval num_timesteps=305000, episode_reward=98.46 +/- 0.69\n",
            "Episode length: 91.40 +/- 15.03\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 91.4        |\n",
            "|    mean_reward          | 98.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 94.7        |\n",
            "|    ep_rew_mean          | 99.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 180         |\n",
            "|    iterations           | 149         |\n",
            "|    time_elapsed         | 1689        |\n",
            "|    total_timesteps      | 305152      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022571903 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.057       |\n",
            "|    explained_variance   | 0.956       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.01        |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | 0.00199     |\n",
            "|    std                  | 0.23        |\n",
            "|    value_loss           | 11.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=305500, episode_reward=99.77 +/- 1.37\n",
            "Episode length: 99.40 +/- 27.97\n",
            "Eval num_timesteps=306000, episode_reward=98.33 +/- 0.61\n",
            "Episode length: 89.60 +/- 30.73\n",
            "Eval num_timesteps=306500, episode_reward=99.24 +/- 0.88\n",
            "Episode length: 95.40 +/- 17.40\n",
            "Eval num_timesteps=307000, episode_reward=98.52 +/- 0.45\n",
            "Episode length: 74.60 +/- 1.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.6        |\n",
            "|    mean_reward          | 98.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 93.7        |\n",
            "|    ep_rew_mean          | 99.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 181         |\n",
            "|    iterations           | 150         |\n",
            "|    time_elapsed         | 1695        |\n",
            "|    total_timesteps      | 307200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005937577 |\n",
            "|    clip_fraction        | 0.0939      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0422      |\n",
            "|    explained_variance   | 0.926       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.21        |\n",
            "|    n_updates            | 1490        |\n",
            "|    policy_gradient_loss | 0.00202     |\n",
            "|    std                  | 0.234       |\n",
            "|    value_loss           | 27.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=307500, episode_reward=98.79 +/- 0.71\n",
            "Episode length: 82.40 +/- 15.40\n",
            "Eval num_timesteps=308000, episode_reward=99.13 +/- 0.69\n",
            "Episode length: 100.00 +/- 13.91\n",
            "Eval num_timesteps=308500, episode_reward=98.04 +/- 0.75\n",
            "Episode length: 95.00 +/- 30.93\n",
            "Eval num_timesteps=309000, episode_reward=99.32 +/- 0.93\n",
            "Episode length: 102.80 +/- 16.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 103         |\n",
            "|    mean_reward          | 99.3        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 93.2        |\n",
            "|    ep_rew_mean          | 99.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 181         |\n",
            "|    iterations           | 151         |\n",
            "|    time_elapsed         | 1701        |\n",
            "|    total_timesteps      | 309248      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029965058 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.03        |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 0.612       |\n",
            "|    n_updates            | 1500        |\n",
            "|    policy_gradient_loss | 0.00548     |\n",
            "|    std                  | 0.235       |\n",
            "|    value_loss           | 9.72        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=309500, episode_reward=98.68 +/- 0.80\n",
            "Episode length: 87.20 +/- 14.22\n",
            "Eval num_timesteps=310000, episode_reward=97.54 +/- 0.76\n",
            "Episode length: 79.40 +/- 12.80\n",
            "Eval num_timesteps=310500, episode_reward=98.89 +/- 0.66\n",
            "Episode length: 92.80 +/- 13.51\n",
            "Eval num_timesteps=311000, episode_reward=98.53 +/- 1.38\n",
            "Episode length: 83.80 +/- 15.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 83.8        |\n",
            "|    mean_reward          | 98.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.5        |\n",
            "|    ep_rew_mean          | 99.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 182         |\n",
            "|    iterations           | 152         |\n",
            "|    time_elapsed         | 1706        |\n",
            "|    total_timesteps      | 311296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010000013 |\n",
            "|    clip_fraction        | 0.092       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.00853     |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 20.5        |\n",
            "|    n_updates            | 1510        |\n",
            "|    policy_gradient_loss | 0.000941    |\n",
            "|    std                  | 0.243       |\n",
            "|    value_loss           | 27.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=311500, episode_reward=99.34 +/- 1.56\n",
            "Episode length: 106.00 +/- 28.23\n",
            "Eval num_timesteps=312000, episode_reward=99.09 +/- 0.65\n",
            "Episode length: 87.20 +/- 13.50\n",
            "Eval num_timesteps=312500, episode_reward=97.73 +/- 0.47\n",
            "Episode length: 73.40 +/- 0.80\n",
            "Eval num_timesteps=313000, episode_reward=98.40 +/- 1.22\n",
            "Episode length: 80.80 +/- 12.01\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 80.8         |\n",
            "|    mean_reward          | 98.4         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 88.3         |\n",
            "|    ep_rew_mean          | 99.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 183          |\n",
            "|    iterations           | 153          |\n",
            "|    time_elapsed         | 1712         |\n",
            "|    total_timesteps      | 313344       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066340882 |\n",
            "|    clip_fraction        | 0.108        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0208      |\n",
            "|    explained_variance   | 0.936        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 16.3         |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | 0.00223      |\n",
            "|    std                  | 0.25         |\n",
            "|    value_loss           | 20.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=313500, episode_reward=98.15 +/- 1.10\n",
            "Episode length: 85.40 +/- 15.20\n",
            "Eval num_timesteps=314000, episode_reward=98.80 +/- 0.92\n",
            "Episode length: 95.80 +/- 28.59\n",
            "Eval num_timesteps=314500, episode_reward=99.81 +/- 0.84\n",
            "Episode length: 99.20 +/- 19.22\n",
            "Eval num_timesteps=315000, episode_reward=98.85 +/- 0.73\n",
            "Episode length: 128.40 +/- 33.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 128         |\n",
            "|    mean_reward          | 98.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87.6        |\n",
            "|    ep_rew_mean          | 99.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 183         |\n",
            "|    iterations           | 154         |\n",
            "|    time_elapsed         | 1718        |\n",
            "|    total_timesteps      | 315392      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004164136 |\n",
            "|    clip_fraction        | 0.0852      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0267     |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.02        |\n",
            "|    n_updates            | 1530        |\n",
            "|    policy_gradient_loss | 0.000178    |\n",
            "|    std                  | 0.247       |\n",
            "|    value_loss           | 10.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=315500, episode_reward=98.30 +/- 1.03\n",
            "Episode length: 80.60 +/- 13.29\n",
            "Eval num_timesteps=316000, episode_reward=97.85 +/- 0.75\n",
            "Episode length: 89.00 +/- 30.52\n",
            "Eval num_timesteps=316500, episode_reward=99.19 +/- 0.63\n",
            "Episode length: 88.00 +/- 14.11\n",
            "Eval num_timesteps=317000, episode_reward=98.41 +/- 0.76\n",
            "Episode length: 86.00 +/- 14.75\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 86           |\n",
            "|    mean_reward          | 98.4         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.3         |\n",
            "|    ep_rew_mean          | 99.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 184          |\n",
            "|    iterations           | 155          |\n",
            "|    time_elapsed         | 1723         |\n",
            "|    total_timesteps      | 317440       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049328036 |\n",
            "|    clip_fraction        | 0.124        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0327      |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 3.9          |\n",
            "|    n_updates            | 1540         |\n",
            "|    policy_gradient_loss | 0.00759      |\n",
            "|    std                  | 0.252        |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=317500, episode_reward=98.09 +/- 0.70\n",
            "Episode length: 73.60 +/- 1.20\n",
            "Eval num_timesteps=318000, episode_reward=98.54 +/- 1.06\n",
            "Episode length: 81.20 +/- 13.09\n",
            "Eval num_timesteps=318500, episode_reward=98.56 +/- 0.73\n",
            "Episode length: 79.80 +/- 12.12\n",
            "Eval num_timesteps=319000, episode_reward=97.66 +/- 0.55\n",
            "Episode length: 73.20 +/- 0.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 73.2        |\n",
            "|    mean_reward          | 97.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87.3        |\n",
            "|    ep_rew_mean          | 99.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 184         |\n",
            "|    iterations           | 156         |\n",
            "|    time_elapsed         | 1728        |\n",
            "|    total_timesteps      | 319488      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008061562 |\n",
            "|    clip_fraction        | 0.0845      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0373     |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 7.91        |\n",
            "|    n_updates            | 1550        |\n",
            "|    policy_gradient_loss | 0.00178     |\n",
            "|    std                  | 0.252       |\n",
            "|    value_loss           | 19          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=319500, episode_reward=98.38 +/- 0.69\n",
            "Episode length: 102.40 +/- 36.42\n",
            "Eval num_timesteps=320000, episode_reward=98.09 +/- 0.27\n",
            "Episode length: 73.00 +/- 0.63\n",
            "Eval num_timesteps=320500, episode_reward=98.57 +/- 1.01\n",
            "Episode length: 94.60 +/- 29.30\n",
            "Eval num_timesteps=321000, episode_reward=99.25 +/- 0.81\n",
            "Episode length: 81.20 +/- 12.11\n",
            "Eval num_timesteps=321500, episode_reward=98.76 +/- 1.07\n",
            "Episode length: 98.00 +/- 32.77\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 98          |\n",
            "|    mean_reward          | 98.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.6        |\n",
            "|    ep_rew_mean          | 99.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 185         |\n",
            "|    iterations           | 157         |\n",
            "|    time_elapsed         | 1735        |\n",
            "|    total_timesteps      | 321536      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008138869 |\n",
            "|    clip_fraction        | 0.096       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0195     |\n",
            "|    explained_variance   | 0.943       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 1560        |\n",
            "|    policy_gradient_loss | 0.00164     |\n",
            "|    std                  | 0.244       |\n",
            "|    value_loss           | 18.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=322000, episode_reward=98.55 +/- 0.69\n",
            "Episode length: 74.00 +/- 1.41\n",
            "Eval num_timesteps=322500, episode_reward=99.45 +/- 1.44\n",
            "Episode length: 81.80 +/- 12.17\n",
            "Eval num_timesteps=323000, episode_reward=98.76 +/- 0.92\n",
            "Episode length: 88.60 +/- 30.29\n",
            "Eval num_timesteps=323500, episode_reward=99.03 +/- 0.77\n",
            "Episode length: 89.60 +/- 30.26\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 89.6        |\n",
            "|    mean_reward          | 99          |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.7        |\n",
            "|    ep_rew_mean          | 99.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 185         |\n",
            "|    iterations           | 158         |\n",
            "|    time_elapsed         | 1740        |\n",
            "|    total_timesteps      | 323584      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007261993 |\n",
            "|    clip_fraction        | 0.044       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0216     |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 21          |\n",
            "|    n_updates            | 1570        |\n",
            "|    policy_gradient_loss | -0.000391   |\n",
            "|    std                  | 0.249       |\n",
            "|    value_loss           | 29.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=324000, episode_reward=99.31 +/- 1.01\n",
            "Episode length: 102.00 +/- 29.82\n",
            "Eval num_timesteps=324500, episode_reward=99.37 +/- 0.99\n",
            "Episode length: 95.80 +/- 28.53\n",
            "Eval num_timesteps=325000, episode_reward=99.40 +/- 1.03\n",
            "Episode length: 101.60 +/- 27.73\n",
            "Eval num_timesteps=325500, episode_reward=98.32 +/- 0.87\n",
            "Episode length: 79.20 +/- 13.41\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 79.2        |\n",
            "|    mean_reward          | 98.3        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 89.2        |\n",
            "|    ep_rew_mean          | 99.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 186         |\n",
            "|    iterations           | 159         |\n",
            "|    time_elapsed         | 1746        |\n",
            "|    total_timesteps      | 325632      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015844997 |\n",
            "|    clip_fraction        | 0.098       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0296     |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 1580        |\n",
            "|    policy_gradient_loss | -0.00156    |\n",
            "|    std                  | 0.25        |\n",
            "|    value_loss           | 22.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=326000, episode_reward=99.13 +/- 0.96\n",
            "Episode length: 75.20 +/- 2.56\n",
            "Eval num_timesteps=326500, episode_reward=99.81 +/- 1.01\n",
            "Episode length: 82.00 +/- 12.31\n",
            "Eval num_timesteps=327000, episode_reward=99.91 +/- 0.73\n",
            "Episode length: 96.80 +/- 28.75\n",
            "Eval num_timesteps=327500, episode_reward=99.43 +/- 1.05\n",
            "Episode length: 96.20 +/- 30.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 96.2        |\n",
            "|    mean_reward          | 99.4        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 92.2        |\n",
            "|    ep_rew_mean          | 99.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 187         |\n",
            "|    iterations           | 160         |\n",
            "|    time_elapsed         | 1751        |\n",
            "|    total_timesteps      | 327680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010472616 |\n",
            "|    clip_fraction        | 0.0645      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0376     |\n",
            "|    explained_variance   | 0.901       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.96        |\n",
            "|    n_updates            | 1590        |\n",
            "|    policy_gradient_loss | -0.00124    |\n",
            "|    std                  | 0.251       |\n",
            "|    value_loss           | 34.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=328000, episode_reward=99.45 +/- 0.93\n",
            "Episode length: 96.00 +/- 29.09\n",
            "Eval num_timesteps=328500, episode_reward=99.58 +/- 0.85\n",
            "Episode length: 96.40 +/- 29.17\n",
            "Eval num_timesteps=329000, episode_reward=99.32 +/- 1.16\n",
            "Episode length: 95.80 +/- 29.43\n",
            "Eval num_timesteps=329500, episode_reward=99.54 +/- 1.60\n",
            "Episode length: 84.80 +/- 20.74\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 84.8         |\n",
            "|    mean_reward          | 99.5         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 91.8         |\n",
            "|    ep_rew_mean          | 99.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 187          |\n",
            "|    iterations           | 161          |\n",
            "|    time_elapsed         | 1757         |\n",
            "|    total_timesteps      | 329728       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0116928015 |\n",
            "|    clip_fraction        | 0.0843       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.0375      |\n",
            "|    explained_variance   | 0.925        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 15.8         |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | 0.00018      |\n",
            "|    std                  | 0.251        |\n",
            "|    value_loss           | 29.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=99.20 +/- 1.49\n",
            "Episode length: 86.80 +/- 17.86\n",
            "Eval num_timesteps=330500, episode_reward=99.80 +/- 1.24\n",
            "Episode length: 89.00 +/- 17.34\n",
            "Eval num_timesteps=331000, episode_reward=99.51 +/- 1.25\n",
            "Episode length: 88.20 +/- 17.93\n",
            "Eval num_timesteps=331500, episode_reward=99.87 +/- 1.04\n",
            "Episode length: 134.40 +/- 31.36\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 134         |\n",
            "|    mean_reward          | 99.9        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.1        |\n",
            "|    ep_rew_mean          | 99.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 188         |\n",
            "|    iterations           | 162         |\n",
            "|    time_elapsed         | 1762        |\n",
            "|    total_timesteps      | 331776      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009996261 |\n",
            "|    clip_fraction        | 0.0955      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0456     |\n",
            "|    explained_variance   | 0.948       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.84        |\n",
            "|    n_updates            | 1610        |\n",
            "|    policy_gradient_loss | 0.00553     |\n",
            "|    std                  | 0.254       |\n",
            "|    value_loss           | 18.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=332000, episode_reward=98.69 +/- 1.01\n",
            "Episode length: 79.20 +/- 13.41\n",
            "Eval num_timesteps=332500, episode_reward=99.49 +/- 1.31\n",
            "Episode length: 89.00 +/- 19.20\n",
            "Eval num_timesteps=333000, episode_reward=98.27 +/- 0.45\n",
            "Episode length: 73.20 +/- 0.98\n",
            "Eval num_timesteps=333500, episode_reward=99.42 +/- 1.37\n",
            "Episode length: 82.20 +/- 12.56\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 82.2        |\n",
            "|    mean_reward          | 99.4        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 92.3        |\n",
            "|    ep_rew_mean          | 99.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 188         |\n",
            "|    iterations           | 163         |\n",
            "|    time_elapsed         | 1768        |\n",
            "|    total_timesteps      | 333824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013600753 |\n",
            "|    clip_fraction        | 0.111       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.0357     |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 1620        |\n",
            "|    policy_gradient_loss | -0.00103    |\n",
            "|    std                  | 0.248       |\n",
            "|    value_loss           | 20.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=334000, episode_reward=99.08 +/- 1.35\n",
            "Episode length: 87.00 +/- 17.70\n",
            "Eval num_timesteps=334500, episode_reward=99.17 +/- 1.07\n",
            "Episode length: 88.00 +/- 16.44\n",
            "Eval num_timesteps=335000, episode_reward=98.60 +/- 1.17\n",
            "Episode length: 81.00 +/- 15.07\n",
            "Eval num_timesteps=335500, episode_reward=98.49 +/- 0.37\n",
            "Episode length: 74.20 +/- 0.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.2        |\n",
            "|    mean_reward          | 98.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.9        |\n",
            "|    ep_rew_mean          | 99.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 189         |\n",
            "|    iterations           | 164         |\n",
            "|    time_elapsed         | 1773        |\n",
            "|    total_timesteps      | 335872      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015213951 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.00738    |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 7.27        |\n",
            "|    n_updates            | 1630        |\n",
            "|    policy_gradient_loss | 0.00364     |\n",
            "|    std                  | 0.241       |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=336000, episode_reward=98.14 +/- 1.14\n",
            "Episode length: 74.80 +/- 2.64\n",
            "Eval num_timesteps=336500, episode_reward=98.76 +/- 1.39\n",
            "Episode length: 92.60 +/- 32.96\n",
            "Eval num_timesteps=337000, episode_reward=97.64 +/- 0.53\n",
            "Episode length: 73.00 +/- 0.63\n",
            "Eval num_timesteps=337500, episode_reward=98.83 +/- 1.34\n",
            "Episode length: 92.00 +/- 16.33\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 92          |\n",
            "|    mean_reward          | 98.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 88.8        |\n",
            "|    ep_rew_mean          | 99.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 189         |\n",
            "|    iterations           | 165         |\n",
            "|    time_elapsed         | 1779        |\n",
            "|    total_timesteps      | 337920      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021518957 |\n",
            "|    clip_fraction        | 0.0887      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0143      |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.83        |\n",
            "|    n_updates            | 1640        |\n",
            "|    policy_gradient_loss | 0.00248     |\n",
            "|    std                  | 0.237       |\n",
            "|    value_loss           | 23.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=338000, episode_reward=98.19 +/- 1.16\n",
            "Episode length: 96.60 +/- 30.83\n",
            "Eval num_timesteps=338500, episode_reward=98.05 +/- 1.32\n",
            "Episode length: 75.00 +/- 3.29\n",
            "Eval num_timesteps=339000, episode_reward=98.49 +/- 1.01\n",
            "Episode length: 86.80 +/- 16.56\n",
            "Eval num_timesteps=339500, episode_reward=97.32 +/- 0.16\n",
            "Episode length: 72.40 +/- 0.49\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 72.4         |\n",
            "|    mean_reward          | 97.3         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 86.4         |\n",
            "|    ep_rew_mean          | 99.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 190          |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 1784         |\n",
            "|    total_timesteps      | 339968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016501809 |\n",
            "|    clip_fraction        | 0.11         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.0173       |\n",
            "|    explained_variance   | 0.913        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 18           |\n",
            "|    n_updates            | 1650         |\n",
            "|    policy_gradient_loss | 0.00588      |\n",
            "|    std                  | 0.24         |\n",
            "|    value_loss           | 29.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=97.57 +/- 0.79\n",
            "Episode length: 74.00 +/- 1.26\n",
            "Eval num_timesteps=340500, episode_reward=96.91 +/- 0.53\n",
            "Episode length: 73.20 +/- 0.40\n",
            "Eval num_timesteps=341000, episode_reward=98.37 +/- 1.33\n",
            "Episode length: 97.40 +/- 31.22\n",
            "Eval num_timesteps=341500, episode_reward=97.04 +/- 1.24\n",
            "Episode length: 74.80 +/- 2.64\n",
            "Eval num_timesteps=342000, episode_reward=97.48 +/- 1.53\n",
            "Episode length: 75.00 +/- 3.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 75          |\n",
            "|    mean_reward          | 97.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 87.2        |\n",
            "|    ep_rew_mean          | 99          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 191         |\n",
            "|    iterations           | 167         |\n",
            "|    time_elapsed         | 1790        |\n",
            "|    total_timesteps      | 342016      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011058445 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.00409     |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.56        |\n",
            "|    n_updates            | 1660        |\n",
            "|    policy_gradient_loss | -0.00152    |\n",
            "|    std                  | 0.242       |\n",
            "|    value_loss           | 24          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=342500, episode_reward=97.64 +/- 0.95\n",
            "Episode length: 89.00 +/- 31.50\n",
            "Eval num_timesteps=343000, episode_reward=98.48 +/- 1.81\n",
            "Episode length: 83.80 +/- 19.63\n",
            "Eval num_timesteps=343500, episode_reward=98.70 +/- 1.34\n",
            "Episode length: 83.60 +/- 17.75\n",
            "Eval num_timesteps=344000, episode_reward=97.68 +/- 0.89\n",
            "Episode length: 89.00 +/- 31.50\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 89          |\n",
            "|    mean_reward          | 97.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 84.4        |\n",
            "|    ep_rew_mean          | 98.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 191         |\n",
            "|    iterations           | 168         |\n",
            "|    time_elapsed         | 1796        |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009149229 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0206      |\n",
            "|    explained_variance   | 0.931       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.38        |\n",
            "|    n_updates            | 1670        |\n",
            "|    policy_gradient_loss | 0.0062      |\n",
            "|    std                  | 0.236       |\n",
            "|    value_loss           | 22.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=344500, episode_reward=98.31 +/- 0.72\n",
            "Episode length: 76.00 +/- 3.03\n",
            "Eval num_timesteps=345000, episode_reward=97.51 +/- 1.29\n",
            "Episode length: 80.60 +/- 15.70\n",
            "Eval num_timesteps=345500, episode_reward=98.37 +/- 1.64\n",
            "Episode length: 88.60 +/- 20.53\n",
            "Eval num_timesteps=346000, episode_reward=98.61 +/- 1.60\n",
            "Episode length: 83.60 +/- 14.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 83.6        |\n",
            "|    mean_reward          | 98.6        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 84.6        |\n",
            "|    ep_rew_mean          | 98.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 192         |\n",
            "|    iterations           | 169         |\n",
            "|    time_elapsed         | 1801        |\n",
            "|    total_timesteps      | 346112      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015211811 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0434      |\n",
            "|    explained_variance   | 0.966       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.26        |\n",
            "|    n_updates            | 1680        |\n",
            "|    policy_gradient_loss | 0.00422     |\n",
            "|    std                  | 0.232       |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=346500, episode_reward=96.62 +/- 0.26\n",
            "Episode length: 73.00 +/- 0.00\n",
            "Eval num_timesteps=347000, episode_reward=97.90 +/- 1.29\n",
            "Episode length: 81.40 +/- 13.06\n",
            "Eval num_timesteps=347500, episode_reward=96.92 +/- 0.75\n",
            "Episode length: 73.80 +/- 1.17\n",
            "Eval num_timesteps=348000, episode_reward=97.82 +/- 1.32\n",
            "Episode length: 81.00 +/- 15.11\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 81          |\n",
            "|    mean_reward          | 97.8        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 83.9        |\n",
            "|    ep_rew_mean          | 98.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 192         |\n",
            "|    iterations           | 170         |\n",
            "|    time_elapsed         | 1806        |\n",
            "|    total_timesteps      | 348160      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013154519 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.0621      |\n",
            "|    explained_variance   | 0.943       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.39        |\n",
            "|    n_updates            | 1690        |\n",
            "|    policy_gradient_loss | 0.000885    |\n",
            "|    std                  | 0.223       |\n",
            "|    value_loss           | 15.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=348500, episode_reward=98.05 +/- 0.26\n",
            "Episode length: 92.20 +/- 34.42\n",
            "Eval num_timesteps=349000, episode_reward=96.64 +/- 0.39\n",
            "Episode length: 73.20 +/- 0.40\n",
            "Eval num_timesteps=349500, episode_reward=97.72 +/- 1.06\n",
            "Episode length: 75.20 +/- 2.23\n",
            "Eval num_timesteps=350000, episode_reward=97.16 +/- 1.50\n",
            "Episode length: 80.80 +/- 15.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 80.8        |\n",
            "|    mean_reward          | 97.2        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 82.7        |\n",
            "|    ep_rew_mean          | 98.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 193         |\n",
            "|    iterations           | 171         |\n",
            "|    time_elapsed         | 1812        |\n",
            "|    total_timesteps      | 350208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022442628 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.102       |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 7.59        |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | 0.00992     |\n",
            "|    std                  | 0.215       |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=350500, episode_reward=97.13 +/- 1.18\n",
            "Episode length: 75.20 +/- 3.92\n",
            "Eval num_timesteps=351000, episode_reward=97.55 +/- 1.24\n",
            "Episode length: 80.60 +/- 12.24\n",
            "Eval num_timesteps=351500, episode_reward=97.97 +/- 1.37\n",
            "Episode length: 87.60 +/- 17.11\n",
            "Eval num_timesteps=352000, episode_reward=97.71 +/- 1.12\n",
            "Episode length: 80.40 +/- 12.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 80.4        |\n",
            "|    mean_reward          | 97.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 82.2        |\n",
            "|    ep_rew_mean          | 98.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 193         |\n",
            "|    iterations           | 172         |\n",
            "|    time_elapsed         | 1817        |\n",
            "|    total_timesteps      | 352256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017123505 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.119       |\n",
            "|    explained_variance   | 0.926       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 15.6        |\n",
            "|    n_updates            | 1710        |\n",
            "|    policy_gradient_loss | 0.000437    |\n",
            "|    std                  | 0.214       |\n",
            "|    value_loss           | 20.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=352500, episode_reward=97.91 +/- 1.39\n",
            "Episode length: 83.80 +/- 16.02\n",
            "Eval num_timesteps=353000, episode_reward=97.44 +/- 1.26\n",
            "Episode length: 81.40 +/- 15.81\n",
            "Eval num_timesteps=353500, episode_reward=96.44 +/- 0.34\n",
            "Episode length: 73.40 +/- 0.49\n",
            "Eval num_timesteps=354000, episode_reward=96.45 +/- 0.33\n",
            "Episode length: 73.20 +/- 0.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 73.2        |\n",
            "|    mean_reward          | 96.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 80.5        |\n",
            "|    ep_rew_mean          | 98.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 194         |\n",
            "|    iterations           | 173         |\n",
            "|    time_elapsed         | 1822        |\n",
            "|    total_timesteps      | 354304      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005702312 |\n",
            "|    clip_fraction        | 0.0724      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.114       |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.71        |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | 0.000325    |\n",
            "|    std                  | 0.217       |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=354500, episode_reward=98.37 +/- 1.33\n",
            "Episode length: 89.00 +/- 18.03\n",
            "Eval num_timesteps=355000, episode_reward=96.74 +/- 0.39\n",
            "Episode length: 73.00 +/- 0.00\n",
            "Eval num_timesteps=355500, episode_reward=98.68 +/- 2.03\n",
            "Episode length: 98.20 +/- 20.89\n",
            "Eval num_timesteps=356000, episode_reward=97.56 +/- 1.03\n",
            "Episode length: 80.60 +/- 12.85\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 80.6       |\n",
            "|    mean_reward          | 97.6       |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 81.5       |\n",
            "|    ep_rew_mean          | 98.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 194        |\n",
            "|    iterations           | 174        |\n",
            "|    time_elapsed         | 1828       |\n",
            "|    total_timesteps      | 356352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04479555 |\n",
            "|    clip_fraction        | 0.136      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.113      |\n",
            "|    explained_variance   | 0.946      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 23.4       |\n",
            "|    n_updates            | 1730       |\n",
            "|    policy_gradient_loss | 0.00757    |\n",
            "|    std                  | 0.214      |\n",
            "|    value_loss           | 17         |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=356500, episode_reward=98.12 +/- 1.50\n",
            "Episode length: 85.00 +/- 17.39\n",
            "Eval num_timesteps=357000, episode_reward=97.11 +/- 1.67\n",
            "Episode length: 79.00 +/- 11.01\n",
            "Eval num_timesteps=357500, episode_reward=97.71 +/- 1.02\n",
            "Episode length: 78.00 +/- 6.29\n",
            "Eval num_timesteps=358000, episode_reward=97.59 +/- 1.47\n",
            "Episode length: 83.00 +/- 13.25\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 83          |\n",
            "|    mean_reward          | 97.6        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 80.4        |\n",
            "|    ep_rew_mean          | 98.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 195         |\n",
            "|    iterations           | 175         |\n",
            "|    time_elapsed         | 1833        |\n",
            "|    total_timesteps      | 358400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010705525 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.145       |\n",
            "|    explained_variance   | 0.922       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 14.9        |\n",
            "|    n_updates            | 1740        |\n",
            "|    policy_gradient_loss | 0.00208     |\n",
            "|    std                  | 0.21        |\n",
            "|    value_loss           | 25.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=358500, episode_reward=96.33 +/- 0.47\n",
            "Episode length: 73.40 +/- 0.49\n",
            "Eval num_timesteps=359000, episode_reward=97.02 +/- 1.19\n",
            "Episode length: 80.40 +/- 14.80\n",
            "Eval num_timesteps=359500, episode_reward=97.67 +/- 0.98\n",
            "Episode length: 80.40 +/- 12.39\n",
            "Eval num_timesteps=360000, episode_reward=97.49 +/- 0.63\n",
            "Episode length: 75.00 +/- 1.67\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 75          |\n",
            "|    mean_reward          | 97.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 80.9        |\n",
            "|    ep_rew_mean          | 98.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 195         |\n",
            "|    iterations           | 176         |\n",
            "|    time_elapsed         | 1839        |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011065759 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.155       |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.55        |\n",
            "|    n_updates            | 1750        |\n",
            "|    policy_gradient_loss | 0.00469     |\n",
            "|    std                  | 0.206       |\n",
            "|    value_loss           | 10.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=360500, episode_reward=98.28 +/- 1.39\n",
            "Episode length: 84.80 +/- 18.15\n",
            "Eval num_timesteps=361000, episode_reward=96.92 +/- 0.74\n",
            "Episode length: 73.60 +/- 1.20\n",
            "Eval num_timesteps=361500, episode_reward=98.19 +/- 1.11\n",
            "Episode length: 82.00 +/- 13.28\n",
            "Eval num_timesteps=362000, episode_reward=96.88 +/- 0.62\n",
            "Episode length: 73.40 +/- 1.36\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 73.4        |\n",
            "|    mean_reward          | 96.9        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.6        |\n",
            "|    ep_rew_mean          | 98.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 196         |\n",
            "|    iterations           | 177         |\n",
            "|    time_elapsed         | 1844        |\n",
            "|    total_timesteps      | 362496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023019634 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.159       |\n",
            "|    explained_variance   | 0.949       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 20.8        |\n",
            "|    n_updates            | 1760        |\n",
            "|    policy_gradient_loss | 0.00706     |\n",
            "|    std                  | 0.207       |\n",
            "|    value_loss           | 16.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=362500, episode_reward=99.05 +/- 1.20\n",
            "Episode length: 89.00 +/- 14.25\n",
            "Eval num_timesteps=363000, episode_reward=98.11 +/- 1.31\n",
            "Episode length: 85.00 +/- 15.92\n",
            "Eval num_timesteps=363500, episode_reward=98.31 +/- 1.33\n",
            "Episode length: 86.00 +/- 16.77\n",
            "Eval num_timesteps=364000, episode_reward=98.98 +/- 1.05\n",
            "Episode length: 88.40 +/- 14.32\n",
            "Eval num_timesteps=364500, episode_reward=97.60 +/- 1.38\n",
            "Episode length: 79.80 +/- 14.12\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 79.8        |\n",
            "|    mean_reward          | 97.6        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78.3        |\n",
            "|    ep_rew_mean          | 98.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 196         |\n",
            "|    iterations           | 178         |\n",
            "|    time_elapsed         | 1850        |\n",
            "|    total_timesteps      | 364544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019815521 |\n",
            "|    clip_fraction        | 0.0978      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.138       |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.19        |\n",
            "|    n_updates            | 1770        |\n",
            "|    policy_gradient_loss | 0.00524     |\n",
            "|    std                  | 0.215       |\n",
            "|    value_loss           | 15.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=365000, episode_reward=98.82 +/- 1.95\n",
            "Episode length: 96.60 +/- 22.10\n",
            "Eval num_timesteps=365500, episode_reward=97.89 +/- 1.08\n",
            "Episode length: 80.00 +/- 13.13\n",
            "Eval num_timesteps=366000, episode_reward=97.57 +/- 0.40\n",
            "Episode length: 73.00 +/- 1.55\n",
            "Eval num_timesteps=366500, episode_reward=98.31 +/- 1.25\n",
            "Episode length: 77.40 +/- 6.62\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 77.4        |\n",
            "|    mean_reward          | 98.3        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.5        |\n",
            "|    ep_rew_mean          | 98.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 197         |\n",
            "|    iterations           | 179         |\n",
            "|    time_elapsed         | 1856        |\n",
            "|    total_timesteps      | 366592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011468004 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.123       |\n",
            "|    explained_variance   | 0.948       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.86        |\n",
            "|    n_updates            | 1780        |\n",
            "|    policy_gradient_loss | -0.00268    |\n",
            "|    std                  | 0.21        |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=367000, episode_reward=96.93 +/- 0.40\n",
            "Episode length: 72.60 +/- 0.49\n",
            "Eval num_timesteps=367500, episode_reward=97.91 +/- 1.15\n",
            "Episode length: 81.40 +/- 16.87\n",
            "Eval num_timesteps=368000, episode_reward=98.17 +/- 1.37\n",
            "Episode length: 87.80 +/- 17.77\n",
            "Eval num_timesteps=368500, episode_reward=97.53 +/- 0.91\n",
            "Episode length: 74.80 +/- 1.72\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.8        |\n",
            "|    mean_reward          | 97.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78.3        |\n",
            "|    ep_rew_mean          | 98.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 180         |\n",
            "|    time_elapsed         | 1861        |\n",
            "|    total_timesteps      | 368640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011959769 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.136       |\n",
            "|    explained_variance   | 0.931       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 5.12        |\n",
            "|    n_updates            | 1790        |\n",
            "|    policy_gradient_loss | 0.00112     |\n",
            "|    std                  | 0.211       |\n",
            "|    value_loss           | 16.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=369000, episode_reward=98.60 +/- 1.10\n",
            "Episode length: 93.40 +/- 16.70\n",
            "Eval num_timesteps=369500, episode_reward=96.94 +/- 0.50\n",
            "Episode length: 72.80 +/- 0.40\n",
            "Eval num_timesteps=370000, episode_reward=97.66 +/- 1.17\n",
            "Episode length: 80.80 +/- 13.15\n",
            "Eval num_timesteps=370500, episode_reward=97.10 +/- 1.07\n",
            "Episode length: 74.80 +/- 4.62\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.8        |\n",
            "|    mean_reward          | 97.1        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.1        |\n",
            "|    ep_rew_mean          | 98.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 198         |\n",
            "|    iterations           | 181         |\n",
            "|    time_elapsed         | 1866        |\n",
            "|    total_timesteps      | 370688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007916128 |\n",
            "|    clip_fraction        | 0.0982      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.164       |\n",
            "|    explained_variance   | 0.933       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 17.9        |\n",
            "|    n_updates            | 1800        |\n",
            "|    policy_gradient_loss | 0.00434     |\n",
            "|    std                  | 0.203       |\n",
            "|    value_loss           | 18.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=371000, episode_reward=98.42 +/- 1.13\n",
            "Episode length: 81.20 +/- 13.56\n",
            "Eval num_timesteps=371500, episode_reward=97.03 +/- 0.32\n",
            "Episode length: 72.20 +/- 0.40\n",
            "Eval num_timesteps=372000, episode_reward=97.17 +/- 0.50\n",
            "Episode length: 72.60 +/- 0.80\n",
            "Eval num_timesteps=372500, episode_reward=98.35 +/- 1.62\n",
            "Episode length: 87.20 +/- 18.79\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 87.2       |\n",
            "|    mean_reward          | 98.3       |\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 78.7       |\n",
            "|    ep_rew_mean          | 98.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 199        |\n",
            "|    iterations           | 182        |\n",
            "|    time_elapsed         | 1872       |\n",
            "|    total_timesteps      | 372736     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00943613 |\n",
            "|    clip_fraction        | 0.13       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | 0.167      |\n",
            "|    explained_variance   | 0.924      |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 11.7       |\n",
            "|    n_updates            | 1810       |\n",
            "|    policy_gradient_loss | 0.00662    |\n",
            "|    std                  | 0.204      |\n",
            "|    value_loss           | 22.6       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=373000, episode_reward=98.67 +/- 0.85\n",
            "Episode length: 81.20 +/- 14.93\n",
            "Eval num_timesteps=373500, episode_reward=97.58 +/- 0.42\n",
            "Episode length: 72.80 +/- 0.75\n",
            "Eval num_timesteps=374000, episode_reward=97.96 +/- 0.89\n",
            "Episode length: 90.20 +/- 33.43\n",
            "Eval num_timesteps=374500, episode_reward=98.00 +/- 0.91\n",
            "Episode length: 74.00 +/- 2.28\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74          |\n",
            "|    mean_reward          | 98          |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 77.5        |\n",
            "|    ep_rew_mean          | 98.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 199         |\n",
            "|    iterations           | 183         |\n",
            "|    time_elapsed         | 1877        |\n",
            "|    total_timesteps      | 374784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010252513 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.142       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 20.9        |\n",
            "|    n_updates            | 1820        |\n",
            "|    policy_gradient_loss | -0.000983   |\n",
            "|    std                  | 0.215       |\n",
            "|    value_loss           | 25.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=375000, episode_reward=98.55 +/- 1.06\n",
            "Episode length: 81.40 +/- 13.53\n",
            "Eval num_timesteps=375500, episode_reward=97.69 +/- 1.20\n",
            "Episode length: 78.80 +/- 13.60\n",
            "Eval num_timesteps=376000, episode_reward=98.84 +/- 1.57\n",
            "Episode length: 82.80 +/- 13.00\n",
            "Eval num_timesteps=376500, episode_reward=97.11 +/- 0.10\n",
            "Episode length: 72.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 72          |\n",
            "|    mean_reward          | 97.1        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 77.8        |\n",
            "|    ep_rew_mean          | 98.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 200         |\n",
            "|    iterations           | 184         |\n",
            "|    time_elapsed         | 1883        |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033713683 |\n",
            "|    clip_fraction        | 0.0965      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.147       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.36        |\n",
            "|    n_updates            | 1830        |\n",
            "|    policy_gradient_loss | 0.00329     |\n",
            "|    std                  | 0.206       |\n",
            "|    value_loss           | 18.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=377000, episode_reward=99.70 +/- 1.83\n",
            "Episode length: 87.20 +/- 20.57\n",
            "Eval num_timesteps=377500, episode_reward=98.04 +/- 0.75\n",
            "Episode length: 73.20 +/- 0.98\n",
            "Eval num_timesteps=378000, episode_reward=99.29 +/- 1.72\n",
            "Episode length: 85.00 +/- 12.90\n",
            "Eval num_timesteps=378500, episode_reward=98.41 +/- 1.25\n",
            "Episode length: 80.40 +/- 15.32\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 80.4        |\n",
            "|    mean_reward          | 98.4        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78          |\n",
            "|    ep_rew_mean          | 98.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 200         |\n",
            "|    iterations           | 185         |\n",
            "|    time_elapsed         | 1888        |\n",
            "|    total_timesteps      | 378880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010076113 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.181       |\n",
            "|    explained_variance   | 0.931       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.74        |\n",
            "|    n_updates            | 1840        |\n",
            "|    policy_gradient_loss | 0.00722     |\n",
            "|    std                  | 0.2         |\n",
            "|    value_loss           | 21.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=379000, episode_reward=98.43 +/- 1.11\n",
            "Episode length: 84.60 +/- 16.28\n",
            "Eval num_timesteps=379500, episode_reward=99.20 +/- 0.76\n",
            "Episode length: 86.40 +/- 15.03\n",
            "Eval num_timesteps=380000, episode_reward=98.30 +/- 0.77\n",
            "Episode length: 78.60 +/- 12.74\n",
            "Eval num_timesteps=380500, episode_reward=98.47 +/- 1.03\n",
            "Episode length: 74.20 +/- 4.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.2        |\n",
            "|    mean_reward          | 98.5        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.5        |\n",
            "|    ep_rew_mean          | 98.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 201         |\n",
            "|    iterations           | 186         |\n",
            "|    time_elapsed         | 1893        |\n",
            "|    total_timesteps      | 380928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007393446 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.176       |\n",
            "|    explained_variance   | 0.95        |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 1850        |\n",
            "|    policy_gradient_loss | 0.00598     |\n",
            "|    std                  | 0.203       |\n",
            "|    value_loss           | 13.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=381000, episode_reward=98.73 +/- 0.92\n",
            "Episode length: 84.60 +/- 16.26\n",
            "Eval num_timesteps=381500, episode_reward=98.98 +/- 1.01\n",
            "Episode length: 91.40 +/- 16.27\n",
            "Eval num_timesteps=382000, episode_reward=98.68 +/- 0.82\n",
            "Episode length: 79.40 +/- 12.88\n",
            "Eval num_timesteps=382500, episode_reward=98.24 +/- 0.89\n",
            "Episode length: 73.00 +/- 3.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 73          |\n",
            "|    mean_reward          | 98.2        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.4        |\n",
            "|    ep_rew_mean          | 98.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 201         |\n",
            "|    iterations           | 187         |\n",
            "|    time_elapsed         | 1899        |\n",
            "|    total_timesteps      | 382976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028561514 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.145       |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 1.5         |\n",
            "|    n_updates            | 1860        |\n",
            "|    policy_gradient_loss | 0.0113      |\n",
            "|    std                  | 0.213       |\n",
            "|    value_loss           | 9.09        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=383000, episode_reward=97.81 +/- 0.70\n",
            "Episode length: 71.60 +/- 1.20\n",
            "Eval num_timesteps=383500, episode_reward=98.36 +/- 1.12\n",
            "Episode length: 79.40 +/- 13.17\n",
            "Eval num_timesteps=384000, episode_reward=99.14 +/- 0.61\n",
            "Episode length: 85.80 +/- 16.18\n",
            "Eval num_timesteps=384500, episode_reward=98.68 +/- 1.32\n",
            "Episode length: 79.60 +/- 12.45\n",
            "Eval num_timesteps=385000, episode_reward=98.45 +/- 0.95\n",
            "Episode length: 79.60 +/- 15.74\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 79.6        |\n",
            "|    mean_reward          | 98.4        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 80.2        |\n",
            "|    ep_rew_mean          | 99          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 202         |\n",
            "|    iterations           | 188         |\n",
            "|    time_elapsed         | 1905        |\n",
            "|    total_timesteps      | 385024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032213323 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.127       |\n",
            "|    explained_variance   | 0.952       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 9.27        |\n",
            "|    n_updates            | 1870        |\n",
            "|    policy_gradient_loss | 0.00462     |\n",
            "|    std                  | 0.213       |\n",
            "|    value_loss           | 14.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=385500, episode_reward=97.61 +/- 0.68\n",
            "Episode length: 71.80 +/- 1.17\n",
            "Eval num_timesteps=386000, episode_reward=98.34 +/- 0.69\n",
            "Episode length: 72.80 +/- 2.23\n",
            "Eval num_timesteps=386500, episode_reward=97.54 +/- 0.28\n",
            "Episode length: 71.00 +/- 0.00\n",
            "Eval num_timesteps=387000, episode_reward=97.72 +/- 0.62\n",
            "Episode length: 71.80 +/- 1.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 71.8        |\n",
            "|    mean_reward          | 97.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.4        |\n",
            "|    ep_rew_mean          | 99          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 202         |\n",
            "|    iterations           | 189         |\n",
            "|    time_elapsed         | 1910        |\n",
            "|    total_timesteps      | 387072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005241334 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.138       |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 2.72        |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | 0.00673     |\n",
            "|    std                  | 0.209       |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=387500, episode_reward=98.78 +/- 1.09\n",
            "Episode length: 74.60 +/- 3.61\n",
            "Eval num_timesteps=388000, episode_reward=97.66 +/- 0.61\n",
            "Episode length: 71.60 +/- 1.20\n",
            "Eval num_timesteps=388500, episode_reward=98.03 +/- 1.27\n",
            "Episode length: 73.00 +/- 4.00\n",
            "Eval num_timesteps=389000, episode_reward=97.96 +/- 0.65\n",
            "Episode length: 72.00 +/- 1.55\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 72          |\n",
            "|    mean_reward          | 98          |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 79.6        |\n",
            "|    ep_rew_mean          | 99.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 203         |\n",
            "|    iterations           | 190         |\n",
            "|    time_elapsed         | 1915        |\n",
            "|    total_timesteps      | 389120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019013844 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.146       |\n",
            "|    explained_variance   | 0.932       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 8.69        |\n",
            "|    n_updates            | 1890        |\n",
            "|    policy_gradient_loss | -0.000203   |\n",
            "|    std                  | 0.208       |\n",
            "|    value_loss           | 18.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=389500, episode_reward=98.66 +/- 1.23\n",
            "Episode length: 90.00 +/- 32.27\n",
            "Eval num_timesteps=390000, episode_reward=98.09 +/- 0.51\n",
            "Episode length: 71.60 +/- 0.80\n",
            "Eval num_timesteps=390500, episode_reward=98.42 +/- 0.57\n",
            "Episode length: 105.60 +/- 42.01\n",
            "Eval num_timesteps=391000, episode_reward=98.61 +/- 1.37\n",
            "Episode length: 79.80 +/- 17.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 79.8        |\n",
            "|    mean_reward          | 98.6        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78.8        |\n",
            "|    ep_rew_mean          | 99.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 203         |\n",
            "|    iterations           | 191         |\n",
            "|    time_elapsed         | 1921        |\n",
            "|    total_timesteps      | 391168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011681567 |\n",
            "|    clip_fraction        | 0.0773      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.162       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.4        |\n",
            "|    n_updates            | 1900        |\n",
            "|    policy_gradient_loss | 0.00194     |\n",
            "|    std                  | 0.206       |\n",
            "|    value_loss           | 23.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=391500, episode_reward=98.89 +/- 0.31\n",
            "Episode length: 106.00 +/- 38.82\n",
            "Eval num_timesteps=392000, episode_reward=98.52 +/- 0.62\n",
            "Episode length: 120.60 +/- 39.70\n",
            "Eval num_timesteps=392500, episode_reward=98.31 +/- 0.97\n",
            "Episode length: 73.00 +/- 2.10\n",
            "Eval num_timesteps=393000, episode_reward=97.68 +/- 0.58\n",
            "Episode length: 71.80 +/- 0.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 71.8        |\n",
            "|    mean_reward          | 97.7        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78.3        |\n",
            "|    ep_rew_mean          | 99          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 204         |\n",
            "|    iterations           | 192         |\n",
            "|    time_elapsed         | 1926        |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028830105 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.168       |\n",
            "|    explained_variance   | 0.897       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 6.29        |\n",
            "|    n_updates            | 1910        |\n",
            "|    policy_gradient_loss | 0.000181    |\n",
            "|    std                  | 0.205       |\n",
            "|    value_loss           | 25.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=393500, episode_reward=97.79 +/- 0.69\n",
            "Episode length: 72.60 +/- 0.80\n",
            "Eval num_timesteps=394000, episode_reward=97.67 +/- 0.57\n",
            "Episode length: 72.20 +/- 0.40\n",
            "Eval num_timesteps=394500, episode_reward=98.28 +/- 0.78\n",
            "Episode length: 73.00 +/- 1.55\n",
            "Eval num_timesteps=395000, episode_reward=98.99 +/- 0.94\n",
            "Episode length: 74.80 +/- 2.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 74.8        |\n",
            "|    mean_reward          | 99          |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78          |\n",
            "|    ep_rew_mean          | 98.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 204         |\n",
            "|    iterations           | 193         |\n",
            "|    time_elapsed         | 1932        |\n",
            "|    total_timesteps      | 395264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011868313 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.18        |\n",
            "|    explained_variance   | 0.9         |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 3.15        |\n",
            "|    n_updates            | 1920        |\n",
            "|    policy_gradient_loss | 0.00309     |\n",
            "|    std                  | 0.201       |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=395500, episode_reward=97.70 +/- 0.87\n",
            "Episode length: 73.00 +/- 0.89\n",
            "Eval num_timesteps=396000, episode_reward=98.12 +/- 1.08\n",
            "Episode length: 73.60 +/- 1.96\n",
            "Eval num_timesteps=396500, episode_reward=98.11 +/- 0.63\n",
            "Episode length: 72.80 +/- 1.17\n",
            "Eval num_timesteps=397000, episode_reward=97.65 +/- 0.62\n",
            "Episode length: 72.60 +/- 1.20\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 72.6         |\n",
            "|    mean_reward          | 97.7         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 78.1         |\n",
            "|    ep_rew_mean          | 98.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 205          |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 1937         |\n",
            "|    total_timesteps      | 397312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019020394 |\n",
            "|    clip_fraction        | 0.0741       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.188        |\n",
            "|    explained_variance   | 0.876        |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 14.1         |\n",
            "|    n_updates            | 1930         |\n",
            "|    policy_gradient_loss | 0.00155      |\n",
            "|    std                  | 0.201        |\n",
            "|    value_loss           | 31.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=397500, episode_reward=97.68 +/- 0.63\n",
            "Episode length: 72.40 +/- 0.80\n",
            "Eval num_timesteps=398000, episode_reward=98.68 +/- 1.21\n",
            "Episode length: 73.80 +/- 1.60\n",
            "Eval num_timesteps=398500, episode_reward=98.06 +/- 1.18\n",
            "Episode length: 72.80 +/- 1.60\n",
            "Eval num_timesteps=399000, episode_reward=98.17 +/- 1.01\n",
            "Episode length: 72.80 +/- 1.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 72.8        |\n",
            "|    mean_reward          | 98.2        |\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 77.9        |\n",
            "|    ep_rew_mean          | 98.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 205         |\n",
            "|    iterations           | 195         |\n",
            "|    time_elapsed         | 1942        |\n",
            "|    total_timesteps      | 399360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021777997 |\n",
            "|    clip_fraction        | 0.0884      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | 0.204       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 11.8        |\n",
            "|    n_updates            | 1940        |\n",
            "|    policy_gradient_loss | 0.00434     |\n",
            "|    std                  | 0.195       |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=399500, episode_reward=98.98 +/- 0.95\n",
            "Episode length: 74.60 +/- 2.06\n",
            "Eval num_timesteps=400000, episode_reward=98.78 +/- 1.38\n",
            "Episode length: 74.20 +/- 2.71\n",
            "Eval num_timesteps=400500, episode_reward=97.59 +/- 0.58\n",
            "Episode length: 72.20 +/- 0.40\n",
            "Eval num_timesteps=401000, episode_reward=97.27 +/- 0.15\n",
            "Episode length: 72.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 72           |\n",
            "|    mean_reward          | 97.3         |\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 78.9         |\n",
            "|    ep_rew_mean          | 98.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 206          |\n",
            "|    iterations           | 196          |\n",
            "|    time_elapsed         | 1948         |\n",
            "|    total_timesteps      | 401408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076427297 |\n",
            "|    clip_fraction        | 0.0621       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | 0.215        |\n",
            "|    explained_variance   | 0.86         |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 15.6         |\n",
            "|    n_updates            | 1950         |\n",
            "|    policy_gradient_loss | 0.000872     |\n",
            "|    std                  | 0.194        |\n",
            "|    value_loss           | 33.1         |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training log data\n",
        "log_df = pd.read_csv('/content/logs/MountainCarContinuous-v0_1.monitor.csv', skiprows=1)\n",
        "\n",
        "# Process the timestamp data\n",
        "log_df['t'] -= log_df['t'].iloc[0]\n",
        "\n",
        "# Plot Mean Rewards\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(log_df['t'], log_df['r'])\n",
        "plt.title('Mean Rewards Over Time')\n",
        "plt.xlabel('Time Elapsed')\n",
        "plt.ylabel('Mean Rewards')\n",
        "plt.show()\n",
        "\n",
        "# Plot Mean Steps\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(log_df['t'], log_df['l'])\n",
        "plt.title('Mean Steps Over Time')\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Mean Steps')\n",
        "plt.show()\n",
        "\n",
        "# Access action sums\n",
        "action_sums = env.action_sums\n",
        "\n",
        "# Plot action sums\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(action_sums)\n",
        "plt.title('Absolute Action Sum Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Absolute Action Sum')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ykFV_Ap4o16",
        "outputId": "d182af36-dd02-4f4d-f484-29a084b9fcd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somehow the reward is decreasing over time, here are some assumptions:\n",
        "1. The agent might be learning to be more energy-efficient, which could lead to a reduction in the energy cost over time. However, this could also mean that the agent is making slower progress towards the goal, which could lead to a lower progress reward. If the reduction in the energy cost is greater than the increase in the progress reward, the total reward could decrease over time.\n",
        "\n",
        "2. The agent might be exploring different strategies to solve the task. During this exploration process, the agent might sometimes take actions that lead to lower rewards. This is a normal part of the learning process in reinforcement learning.\n",
        "\n",
        "3. The environment might be stochastic, meaning that there is some randomness in how the state of the environment changes in response to the agent's actions. This could lead to variability in the rewards over time."
      ],
      "metadata": {
        "id": "FjGPKYAhOdmY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ETYaxZBeYKM",
        "outputId": "3d2a9774-e3c1-48e6-ceab-e87615c1275c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the trained agent on the environment\n",
        "obs = env.reset()\n",
        "frames = []  # to store the frames\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, terminated, info = env.step(action)\n",
        "    frames.append(env.render(mode=\"rgb_array\"))  # capture the frame\n",
        "    if terminated:\n",
        "        obs = env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uiEgzll4tdr",
        "outputId": "80941489-fc3a-4ae4-9f4c-3d8f35b76c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an animation\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "\n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "\n",
        "ani = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "ani.save('animation.gif', writer='imagemagick')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "fEcf_hUxLpab",
        "outputId": "aaa0d662-47ae-4b5f-91df-ecb804c7104e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}